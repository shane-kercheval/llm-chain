{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set above. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relevant documents to include in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3042077414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'This is a document. It has information related to the question I want to ask.',\n",
    "    'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "    'Here is another document.',\n",
    "]\n",
    "question = \"What is the answer for the codeword `flibberwump`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.base import Document\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")\n",
    "document_index.add_documents(docs=[Document(content=x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer for the codeword `flibberwump` is `hanzo`.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_template = DocSearchTemplate(doc_index=document_index, n_docs=1)\n",
    "\n",
    "# A chain is simply a collection of callables where the output of the previous callable matches\n",
    "# the input to the next callable.\n",
    "# Below, the input to the `DocSearchTemplate` is a string (the question) and the output is a\n",
    "# string (the prompt); and the input to `OpenAIChat` is a string (the prompt).\n",
    "# Question (str) -> Prompt (str) -> Answer (str)\n",
    "chain = Chain(links=[\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 171\n",
      "Cost: $0.0002492\n"
     ]
    }
   ],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:01:34; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 41; cost: $0.000016\n",
      "timestamp: 2023-06-13 02:01:34; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 17; cost: $0.000007\n",
      "timestamp: 2023-06-13 02:01:36; prompt: \"Answer the question ...\"; response: \"The answer for the c...\";  total_tokens: 113; cost: $0.000226 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# you can see the individual costs for the embeddings and the chat\n",
    "# The embeddings model has 2 records in its history; \n",
    "# 1 to embed the original docs and the other to embed the question passed into the chain\n",
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
      "\n",
      "Here is the information:\n",
      "\n",
      "```\n",
      "The codeword is `flibberwump`; the answer is `hanzo`.\n",
      "```\n",
      "\n",
      "Here is the question:\n",
      "\n",
      "What is the answer for the codeword `flibberwump`?\n",
      "\n",
      "response: The answer for the codeword `flibberwump` is `hanzo`.\n",
      "cost: 0.000226\n"
     ]
    }
   ],
   "source": [
    "# The chat has model has 1 record in its history\n",
    "chat_model = chain[1]\n",
    "print(f\"prompt: {chat_model._history[0].prompt}\")\n",
    "print(f\"response: {chat_model._history[0].response}\")\n",
    "print(f\"cost: {chat_model._history[0].cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text():\n",
    "    return [\n",
    "        'This is a document. It has information related to the question I want to ask.',\n",
    "        'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "        'Here is another document.',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer for the codeword `flibberwump` is `hanzo`.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <nothing> -> list[str]\n",
    "# list[str] -> None\n",
    "# <ignored> -> str\n",
    "# str -> str\n",
    "# str -> str\n",
    "chain = Chain(links=[\n",
    "    load_text,\n",
    "    lambda texts: document_index.add_documents(docs=[Document(content=x) for x in texts]),\n",
    "    lambda _: \"What is the answer for the codeword `flibberwump`?\",\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 171\n",
      "Cost: $0.0002492\n"
     ]
    }
   ],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:23:09; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 41; cost: $0.000016\n",
      "timestamp: 2023-06-13 02:23:09; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 17; cost: $0.000007\n",
      "timestamp: 2023-06-13 02:23:11; prompt: \"Answer the question ...\"; response: \"The answer for the c...\";  total_tokens: 113; cost: $0.000226 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Shane! How can I assist you today?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo', temperature=0)\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000072\n",
      "    Total Tokens: 36\n",
      "    Total Prompt Tokens: 26\n",
      "    Total Response Tokens: 10\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageMetaData: timestamp: 2023-06-13 02:13:15; prompt: \"Hi, my name is Shane...\"; response: \"Hello Shane! How can...\";  total_tokens: 36; cost: $0.000072 metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "previous prompt: Hi, my name is Shane.\n",
      "previous response: Hello Shane! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:13:15; prompt: \"Hi, my name is Shane...\"; response: \"Hello Shane! How can...\";  total_tokens: 36; cost: $0.000072 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Shane.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Shane.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:13:15; prompt: \"Hi, my name is Shane...\"; response: \"Hello Shane! How can...\";  total_tokens: 36; cost: $0.000072 metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "timestamp: 2023-06-13 02:16:42; prompt: \"Do you remember my n...\"; response: \"Yes, your name is Sh...\";  total_tokens: 59; cost: $0.000118 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Shane.'},\n",
       " {'role': 'assistant', 'content': 'Hello Shane! How can I assist you today?'},\n",
       " {'role': 'user', 'content': 'Do you remember my name?'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageMetaData: timestamp: 2023-06-13 02:16:42; prompt: \"Do you remember my n...\"; response: \"Yes, your name is Sh...\";  total_tokens: 59; cost: $0.000118 metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "previous prompt: Do you remember my name?\n",
      "previous response: Yes, your name is Shane.\n"
     ]
    }
   ],
   "source": [
    "# You can get the last MessageMetaData via: \n",
    "print(f\"MessageMetaData: {chat.previous_message}\")\n",
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000190\n",
      "    Total Tokens: 95\n",
      "    Total Prompt Tokens: 78\n",
      "    Total Response Tokens: 17\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `OpenAIChat` model has a `memory_strategy` parameter and takes a `MemoryBuffer` class. A `MemoryBuffer` class is a callable that takes a `list[MessageMetaData]` (i.e. from the `model.history` property) and also returns a `list[MessageMetaData]` serving as the model's memory (i.e. a list containing the messages that will be sent to the model along with the new prompt). This allows the end user to easily define a memory strategy of their own (e.g. keep the first message and the last `n` messages).\n",
    "\n",
    "One Example of a `MemoryBuffer` is a `MemoryBufferMessageWindow` class where you can specify the last `n` messages that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Shane! How can I assist you today?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.memory import MemoryBufferMessageWindow\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    memory_strategy=MemoryBufferMessageWindow(last_n_messages=0),  # no memory\n",
    ")\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000072\n",
      "    Total Tokens: 36\n",
      "    Total Prompt Tokens: 26\n",
      "    Total Response Tokens: 10\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Shane.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI language model, I don't have the ability to remember specific information about individual users. However, I'm always here to assist you with any questions or tasks you may have.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MessageMetaData(prompt='Hi, my name is Shane.', response='Hello Shane! How can I assist you today?', metadata={'model_name': 'gpt-3.5-turbo'}, prompt_tokens=26, response_tokens=10, total_tokens=36, cost=7.2e-05),\n",
       " MessageMetaData(prompt='Do you remember my name?', response=\"I'm sorry, but as an AI language model, I don't have the ability to remember specific information about individual users. However, I'm always here to assist you with any questions or tasks you may have.\", metadata={'model_name': 'gpt-3.5-turbo'}, prompt_tokens=25, response_tokens=43, total_tokens=68, cost=0.000136)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we still have access to the full history, but the ChatGPT didn't use any of it.\n",
    "chat._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Do you remember my name?'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000208\n",
      "    Total Tokens: 104\n",
      "    Total Prompt Tokens: 51\n",
      "    Total Response Tokens: 53\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
