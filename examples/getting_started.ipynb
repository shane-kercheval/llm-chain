{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relevant documents to include in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'This is a document. It has information related to the question I want to ask.',\n",
    "    'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "    'Here is another document.',\n",
    "]\n",
    "question = \"What is the answer for the codeword `flibberwump`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.base import Document\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")\n",
    "document_index.add(docs=[Document(content=x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_template = DocSearchTemplate(doc_index=document_index, n_docs=1)\n",
    "\n",
    "# A chain is simply a collection of callables where the output of the previous callable matches\n",
    "# the input to the next callable.\n",
    "# Below, the input to the `DocSearchTemplate` is a string (the question) and the output is a\n",
    "# string (the prompt); and the input to `OpenAIChat` is a string (the prompt).\n",
    "# Question (str) -> Prompt (str) -> Answer (str)\n",
    "chain = Chain(links=[\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the individual costs for the embeddings and the chat\n",
    "# The embeddings model has 2 records in its history; \n",
    "# 1 to embed the original docs and the other to embed the question passed into the chain\n",
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat has model has 1 record in its history\n",
    "chat_model = chain[1]\n",
    "print(f\"prompt: {chat_model._history[0].prompt}\")\n",
    "print(f\"response: {chat_model._history[0].response}\")\n",
    "print(f\"cost: {chat_model._history[0].cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text():\n",
    "    return [\n",
    "        'This is a document. It has information related to the question I want to ask.',\n",
    "        'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "        'Here is another document.',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <nothing> -> list[str]\n",
    "# list[str] -> None\n",
    "# <ignored> -> str\n",
    "# str -> str\n",
    "# str -> str\n",
    "chain = Chain(links=[\n",
    "    load_text,\n",
    "    lambda texts: document_index.add(docs=[Document(content=x) for x in texts]),\n",
    "    lambda _: \"What is the answer for the codeword `flibberwump`?\",\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "\n",
    "doc = html_page_loader(url='https://python.langchain.com/en/latest/modules/agents.html')\n",
    "doc.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "chat_model_name = 'gpt-3.5-turbo'\n",
    "emb_model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# document_index is used to store and retrieve documents scraped from the URL defined below\n",
    "# the __call__ function calls add() or search() based on input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name=emb_model_name))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "chat = OpenAIChat(model_name=chat_model_name)\n",
    "# converts a string to a list containing a single Document object\n",
    "text_to_docs = lambda x: [Document(content=x.replace('\\n', ' '))]\n",
    "\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "ask_question_1 = lambda _: \"What is a langchain `Agent`?\"\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "question_2 = lambda x: f'Summarize the following in less than 10 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    html_page_loader,\n",
    "    text_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    ask_question_1,\n",
    "    prompt_template,\n",
    "    chat,\n",
    "    question_2,\n",
    "    chat,\n",
    "])\n",
    "\n",
    "response = chain('https://python.langchain.com/en/latest/modules/agents.html')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search via DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import duckduckgo_search, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# document_index is used to store and retrieve documents scraped from the URL defined below\n",
    "# the __call__ function calls add() or search() based on the input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "question_1 = \"What is a langchain `Agent`?\"\n",
    "\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "ask_question_1 = lambda _: question_1\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "ask_question_2 = lambda x: f'Summarize the following in less than 10 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    ask_question_1,\n",
    "    prompt_template,\n",
    "    chat,\n",
    "    ask_question_2,\n",
    "    chat,\n",
    "])\n",
    "\n",
    "response = chain(question_1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_question.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallableClass:\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "    \n",
    "    def __call__(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        return self.value\n",
    "\n",
    "\n",
    "# Create an instance of CallableClass\n",
    "my_callable = CallableClass()\n",
    "\n",
    "# Call the instance without passing a value\n",
    "result1 = my_callable()  # Returns: None\n",
    "print(result1)\n",
    "\n",
    "# Call the instance and pass a value\n",
    "my_callable(\"Hello, World!\")\n",
    "\n",
    "# Call the instance again without passing a value\n",
    "result2 = my_callable()  # Returns: \"Hello, World!\"\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LangChain Agent drives decision-making, accesses tools, and builds adaptive applications with context-specific responses.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import duckduckgo_search, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain, Value\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# the __call__ function calls add() or search() based on the input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "\n",
    "initial_question = Value()\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    initial_question,\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    initial_question,\n",
    "    prompt_template,\n",
    "    chat,\n",
    "    question_2,\n",
    "    chat,\n",
    "])\n",
    "\n",
    "response = chain(\"What is a langchain agent?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0019\n",
      "Tokens: 8,319\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "f LangChain to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases.What is a LangChain Agent?A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses. They are especially useful when there's an unknown chain of interactions that de\n",
       "\n",
       " LangChain enables chains to interact with external data sources to gather data for the generation step. For example, it can help with summarizing long texts or answering questions using specific data sources.Agents: An agent lets an LLM make decisions about actions, take those actions, check the results, and keep going until the job's done. LangChain provides a standard interface for agents, a variety of agents to choose from, and examples of end-to-end agents.Memory: LangChain has a standard i\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain agent?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent drives decision-making, accesses tools, and builds adaptive applications with context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LangChain Document Loader is a versatile tool that loads text from various sources and transforms data for language models.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain(\"What is a langchain document loader?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0048\n",
      "Tokens: 16,495\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "f LangChain to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases.What is a LangChain Agent?A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses. They are especially useful when there's an unknown chain of interactions that de\n",
       "\n",
       " LangChain enables chains to interact with external data sources to gather data for the generation step. For example, it can help with summarizing long texts or answering questions using specific data sources.Agents: An agent lets an LLM make decisions about actions, take those actions, check the results, and keep going until the job's done. LangChain provides a standard interface for agents, a variety of agents to choose from, and examples of end-to-end agents.Memory: LangChain has a standard i\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain agent?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent drives decision-making, accesses tools, and builds adaptive applications with context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "LangChain Indexes: Document Loaders                                                                Home About Contact      Sign in Subscribe           LangChain     Featured  LangChain Indexes: Document Loaders Dive into the world of LangChain Document Loaders, understand how they work to transform and load text from various sources and learn how to use them in your language modeling tasks.           David Gentile  May 25, 2023 â€¢ 7 min read          Welcome to the LangChain introduction series. \n",
       "\n",
       "es. They are versatile tools that can handle various data formats and transform them into a standard structure that language models can easily process.This guide aims to explain LangChain Document Loaders in-depth, enabling you to make the most of them in your LLM applications.Understanding LangChain Document LoadersThe first concept to understand is what Langchain calls a Document. It really does not get more straightforward as a Document has two fields:page_content (string): the raw text of th\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain document loader?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Document Loader is a versatile tool that can handle various data formats and transform them into a standard structure that language models can easily process. It loads text from various sources and is used in language modeling tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Document Loader is a versatile tool that can handle various data formats and transform them into a standard structure that language models can easily process. It loads text from various sources and is used in language modeling tasks.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Document Loader is a versatile tool that loads text from various sources and transforms data for language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo', temperature=0)\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the last MessageMetaData via: \n",
    "print(f\"MessageMetaData: {chat.previous_message}\")\n",
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `OpenAIChat` model has a `memory_strategy` parameter and takes a `MemoryBuffer` class. A `MemoryBuffer` class is a callable that takes a `list[MessageMetaData]` (i.e. from the `model.history` property) and also returns a `list[MessageMetaData]` serving as the model's memory (i.e. a list containing the messages that will be sent to the model along with the new prompt). This allows the end user to easily define a memory strategy of their own (e.g. keep the first message and the last `n` messages).\n",
    "\n",
    "One Example of a `MemoryBuffer` is a `MemoryBufferMessageWindow` class where you can specify the last `n` messages that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.memory import MemoryBufferMessageWindow\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    memory_strategy=MemoryBufferMessageWindow(last_n_messages=0),  # no memory\n",
    ")\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have access to the full history, but the ChatGPT didn't use any of it.\n",
    "chat._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
