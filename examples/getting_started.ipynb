{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set above. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relevant documents to include in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3042077414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'This is a document. It has information related to the question I want to ask.',\n",
    "    'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "    'Here is another document.',\n",
    "]\n",
    "question = \"What is the answer for the codeword `flibberwump`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChromaDocumentIndex' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# create a document index (i.e. vector database) and add the text from above.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m document_index \u001b[39m=\u001b[39m ChromaDocumentIndex(\n\u001b[1;32m      7\u001b[0m     embeddings_model\u001b[39m=\u001b[39mOpenAIEmbeddings(model_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m document_index\u001b[39m.\u001b[39;49madd(docs\u001b[39m=\u001b[39m[Document(content\u001b[39m=\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m texts])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChromaDocumentIndex' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "from llm_chain.base import Document\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")\n",
    "document_index.add(docs=[Document(content=x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_template = DocSearchTemplate(doc_index=document_index, n_docs=1)\n",
    "\n",
    "# A chain is simply a collection of callables where the output of the previous callable matches\n",
    "# the input to the next callable.\n",
    "# Below, the input to the `DocSearchTemplate` is a string (the question) and the output is a\n",
    "# string (the prompt); and the input to `OpenAIChat` is a string (the prompt).\n",
    "# Question (str) -> Prompt (str) -> Answer (str)\n",
    "chain = Chain(links=[\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the individual costs for the embeddings and the chat\n",
    "# The embeddings model has 2 records in its history; \n",
    "# 1 to embed the original docs and the other to embed the question passed into the chain\n",
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat has model has 1 record in its history\n",
    "chat_model = chain[1]\n",
    "print(f\"prompt: {chat_model._history[0].prompt}\")\n",
    "print(f\"response: {chat_model._history[0].response}\")\n",
    "print(f\"cost: {chat_model._history[0].cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text():\n",
    "    return [\n",
    "        'This is a document. It has information related to the question I want to ask.',\n",
    "        'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "        'Here is another document.',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <nothing> -> list[str]\n",
    "# list[str] -> None\n",
    "# <ignored> -> str\n",
    "# str -> str\n",
    "# str -> str\n",
    "chain = Chain(links=[\n",
    "    load_text,\n",
    "    lambda texts: document_index.add(docs=[Document(content=x) for x in texts]),\n",
    "    lambda _: \"What is the answer for the codeword `flibberwump`?\",\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "\n",
    "doc = html_page_loader(url='https://python.langchain.com/en/latest/modules/agents.html')\n",
    "doc.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "The server had an error while processing your request. Sorry about that!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m# each link is a callable where the output of one link is the input to the next\u001b[39;00m\n\u001b[1;32m     29\u001b[0m chain \u001b[39m=\u001b[39m Chain(links\u001b[39m=\u001b[39m[\n\u001b[1;32m     30\u001b[0m     html_page_loader,\n\u001b[1;32m     31\u001b[0m     text_to_docs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     chat,\n\u001b[1;32m     39\u001b[0m ])\n\u001b[0;32m---> 41\u001b[0m response \u001b[39m=\u001b[39m chain(\u001b[39m'\u001b[39;49m\u001b[39mhttps://python.langchain.com/en/latest/modules/agents.html\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m response\n",
      "File \u001b[0;32m/code/llm_chain/chains.py:22\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_links) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_links[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m---> 22\u001b[0m         result \u001b[39m=\u001b[39m link(result)\n\u001b[1;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/code/llm_chain/base.py:264\u001b[0m, in \u001b[0;36mDocumentIndex.__call__\u001b[0;34m(self, value, n_results)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"TODO.\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(docs\u001b[39m=\u001b[39;49mvalue)\n\u001b[1;32m    265\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Document)\n\u001b[1;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch(doc\u001b[39m=\u001b[39mvalue, n_results\u001b[39m=\u001b[39mn_results)\n",
      "File \u001b[0;32m/code/llm_chain/indexes.py:19\u001b[0m, in \u001b[0;36mChromaDocumentIndex.add\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(\u001b[39mself\u001b[39m, docs: \u001b[39mlist\u001b[39m[Document]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"TODO.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embeddings_model(docs\u001b[39m=\u001b[39;49mdocs)\n\u001b[1;32m     20\u001b[0m     metadatas \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mmetadata \u001b[39mor\u001b[39;00m {} \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m docs]\n\u001b[1;32m     21\u001b[0m     documents \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcontent \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m docs]\n",
      "File \u001b[0;32m/code/llm_chain/base.py:131\u001b[0m, in \u001b[0;36mEmbeddingsModel.__call__\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, docs: \u001b[39mlist\u001b[39m[Document]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"TODO.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     embeddings, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(docs\u001b[39m=\u001b[39;49mdocs)\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_history\u001b[39m.\u001b[39mappend(metadata)\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/code/llm_chain/models.py:42\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._run\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m     41\u001b[0m texts \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_prep(x\u001b[39m.\u001b[39mcontent) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m---> 42\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m \u001b[39m=\u001b[39;49m texts, model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name)\n\u001b[1;32m     43\u001b[0m total_tokens \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39musage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     44\u001b[0m embeddings \u001b[39m=\u001b[39m [x[\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_resources/embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: The server had an error while processing your request. Sorry about that!"
     ]
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "chat_model_name = 'gpt-3.5-turbo'\n",
    "emb_model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# document_index is used to store and retrieve documents scraped from the URL defined below\n",
    "# the __call__ function calls add() or search() based on input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name=emb_model_name))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "chat = OpenAIChat(model_name=chat_model_name)\n",
    "# converts a string to a list containing a single Document object\n",
    "text_to_docs = lambda x: [Document(content=x.replace('\\n', ' '))]\n",
    "\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "ask_question_1 = lambda _: \"What is a langchain `Agent`?\"\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "question_2 = lambda x: f'Summarize the following in less than 10 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    html_page_loader,\n",
    "    text_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    ask_question_1,\n",
    "    prompt_template,\n",
    "    chat,\n",
    "    question_2,\n",
    "    chat,\n",
    "])\n",
    "\n",
    "response = chain('https://python.langchain.com/en/latest/modules/agents.html')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search via DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Agent drives adaptive decision-making in complex apps.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import duckduckgo_search, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# document_index is used to store and retrieve documents scraped from the URL defined below\n",
    "# the __call__ function calls add() or search() based on the input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "question_1 = \"What is a langchain `Agent`?\"\n",
    "\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "ask_question_1 = lambda _: question_1\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "ask_question_2 = lambda x: f'Summarize the following in less than 10 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    ask_question_1,\n",
    "    prompt_template,\n",
    "    chat,\n",
    "    ask_question_2,\n",
    "    chat,\n",
    "])\n",
    "\n",
    "response = chain(question_1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0021\n",
      "Tokens: 10,624\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-14 03:07:15.902; cost: $0.000991; total_tokens: 9,910; metadata: {'model_name': 'text-embedding-ada-002'}\n",
      "timestamp: 2023-06-14 03:07:16.135; cost: $0.000001; total_tokens: 9; metadata: {'model_name': 'text-embedding-ada-002'}\n",
      "timestamp: 2023-06-14 03:07:19.003; prompt: \"Answer the question ...\"; response: \"A LangChain Agent is...\";  cost: $0.000491; total_tokens: 311; metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "timestamp: 2023-06-14 03:07:20.071; prompt: \"Summarize the follow...\"; response: \"LangChain Agent driv...\";  cost: $0.000596; total_tokens: 394; metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "f LangChain to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases.What is a LangChain Agent?A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses. They are especially useful when there's an unknown chain of interactions that de\n",
       "\n",
       " LangChain enables chains to interact with external data sources to gather data for the generation step. For example, it can help with summarizing long texts or answering questions using specific data sources.Agents: An agent lets an LLM make decisions about actions, take those actions, check the results, and keep going until the job's done. LangChain provides a standard interface for agents, a variety of agents to choose from, and examples of end-to-end agents.Memory: LangChain has a standard i\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain `Agent`?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 10 words: \"A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > LangChain Agent drives adaptive decision-making in applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo', temperature=0)\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the last MessageMetaData via: \n",
    "print(f\"MessageMetaData: {chat.previous_message}\")\n",
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `OpenAIChat` model has a `memory_strategy` parameter and takes a `MemoryBuffer` class. A `MemoryBuffer` class is a callable that takes a `list[MessageMetaData]` (i.e. from the `model.history` property) and also returns a `list[MessageMetaData]` serving as the model's memory (i.e. a list containing the messages that will be sent to the model along with the new prompt). This allows the end user to easily define a memory strategy of their own (e.g. keep the first message and the last `n` messages).\n",
    "\n",
    "One Example of a `MemoryBuffer` is a `MemoryBufferMessageWindow` class where you can specify the last `n` messages that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.memory import MemoryBufferMessageWindow\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    memory_strategy=MemoryBufferMessageWindow(last_n_messages=0),  # no memory\n",
    ")\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have access to the full history, but the ChatGPT didn't use any of it.\n",
    "chat._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
