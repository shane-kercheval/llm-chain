{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relevant documents to include in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'This is a document. It has information related to the question I want to ask.',\n",
    "    'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "    'Here is another document.',\n",
    "]\n",
    "question = \"What is the answer for the codeword `flibberwump`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.base import Document\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")\n",
    "document_index.add(docs=[Document(content=x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_template = DocSearchTemplate(doc_index=document_index, n_docs=1)\n",
    "\n",
    "# A chain is simply a collection of callables where the output of the previous callable matches\n",
    "# the input to the next callable.\n",
    "# Below, the input to the `DocSearchTemplate` is a string (the question) and the output is a\n",
    "# string (the prompt); and the input to `OpenAIChat` is a string (the prompt).\n",
    "# Question (str) -> Prompt (str) -> Answer (str)\n",
    "chain = Chain(links=[\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the individual costs for the embeddings and the chat\n",
    "# The embeddings model has 2 records in its history; \n",
    "# 1 to embed the original docs and the other to embed the question passed into the chain\n",
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat has model has 1 record in its history\n",
    "chat_model = chain[1]\n",
    "print(f\"prompt: {chat_model._history[0].prompt}\")\n",
    "print(f\"response: {chat_model._history[0].response}\")\n",
    "print(f\"cost: {chat_model._history[0].cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text():\n",
    "    return [\n",
    "        'This is a document. It has information related to the question I want to ask.',\n",
    "        'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "        'Here is another document.',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <nothing> -> list[str]\n",
    "# list[str] -> None\n",
    "# <ignored> -> str\n",
    "# str -> str\n",
    "# str -> str\n",
    "chain = Chain(links=[\n",
    "    load_text,\n",
    "    lambda texts: document_index.add(docs=[Document(content=x) for x in texts]),\n",
    "    lambda _: \"What is the answer for the codeword `flibberwump`?\",\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "\n",
    "doc = html_page_loader(url='https://python.langchain.com/en/latest/modules/agents.html')\n",
    "doc.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "chat_model_name = 'gpt-3.5-turbo'\n",
    "emb_model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# document_index is used to store and retrieve documents scraped from the URL defined below\n",
    "# the __call__ function calls add() or search() based on input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name=emb_model_name))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "chat = OpenAIChat(model_name=chat_model_name)\n",
    "# converts a string to a list containing a single Document object\n",
    "text_to_docs = lambda x: [Document(content=x.replace('\\n', ' '))]\n",
    "\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "ask_question_1 = lambda _: \"What is a langchain `Agent`?\"\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "question_2 = lambda x: f'Summarize the following in less than 10 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    html_page_loader,\n",
    "    text_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    ask_question_1,\n",
    "    prompt_template,\n",
    "    chat,\n",
    "    question_2,\n",
    "    chat,\n",
    "])\n",
    "\n",
    "response = chain('https://python.langchain.com/en/latest/modules/agents.html')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search via DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided information suggests that to fine-tune an OpenAI Chat model, one can follow the steps outlined in a blog post that explains the process using Python."
     ]
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import duckduckgo_search, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# document_index is used to store and retrieve documents scraped from the URL defined below\n",
    "# the __call__ function calls add() or search() based on the input\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "# prompt_template is retrieves the most relevant docs and stuffs them into the prompt\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo', streaming_callback=lambda x: print(x, end=''))\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "question_1 = \"How do I fine-tune an OpenAI Chat model?\"\n",
    "\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "# The first question uses the context from the url via the prior links\n",
    "ask_question_1 = lambda _: question_1\n",
    "# the second question uses the answer from ChatGPT as part of the prompt\n",
    "ask_question_2 = lambda x: f'Summarize the following:\\n\\n{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    ask_question_1,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    ask_question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "response = chain(question_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided information suggests that to fine-tune an OpenAI Chat model, one can follow the steps outlined in a blog post that explains the process using Python.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0027\n",
      "Tokens: 20,328\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-17 05:00:11.604; cost: $0.001989; total_tokens: 19,892; metadata: {'model_name': 'text-embedding-ada-002'}\n",
      "timestamp: 2023-06-17 05:00:12.068; cost: $0.000001; total_tokens: 12; metadata: {'model_name': 'text-embedding-ada-002'}\n",
      "timestamp: 2023-06-17 05:00:14.224; prompt: \"Answer the question ...\"; response: \"To fine-tune an Open...\";  cost: $0.000508; total_tokens: 325; metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "timestamp: 2023-06-17 05:00:15.871; prompt: \"Summarize the follow...\"; response: \"The provided informa...\";  cost: $0.000164; total_tokens: 99; metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "on In this blog post, we've shown you how to fine-tune an OpenAI model using Python. Fine-tuning allows you to adapt the powerful GPT models to your specific tasks, making them more relevant and useful for your applications. By following these steps, you can leverage the capabilities of OpenAI models to create more accurate and task-specific NLP solutions. Happy fine-tuning!   openaichatgptGPT       Saadullah Aleem                            Previous Previous Vectors, Vector Search, and Vector D\n",
       "\n",
       "enerate human-like text and perform various NLP tasks, such as text generation, translation, summarization, and question-answering. They can even be trained to create vector embeddings to be used in vector search. What is Fine-tuning? Fine-tuning an OpenAI model allows you to adapt the model to your specific task, improving its performance and making it more relevant to your application. In this blog post, we will walk you through the process of fine-tuning an OpenAI model using Python. Prerequi\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "How do I fine-tune an OpenAI Chat model?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > To fine-tune an OpenAI Chat model, you can follow the steps outlined in the blog post mentioned in the provided information, which explains the process of fine-tuning an OpenAI model using Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following information:\n",
       "\n",
       "To fine-tune an OpenAI Chat model, you can follow the steps outlined in the blog post mentioned in the provided information, which explains the process of fine-tuning an OpenAI model using Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > The provided information suggests that to fine-tune an OpenAI Chat model, one can follow the steps outlined in a blog post that explains the process using Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is complex and subjective, with no definitive answer, and may distract from actually living it."
     ]
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import duckduckgo_search, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain, Value\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "streaming_callback = lambda x: print(x.response, end='')\n",
    "streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo', streaming_callback=streaming_callback)\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "initial_question = Value()\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    initial_question,\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    initial_question,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "\n",
    "response = chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is complex and subjective, with no definitive answer, and may distract from actually living it.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0053\n",
      "Tokens: 45,536\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "the meaning of life at all? To what purpose is it played, this farce in which everything that is essential is irrevocably fixed and determined?[5]Questions about the meaning of life have been expressed in a broad variety of other ways, including: What is the meaning of life? What's it all about? Who are we?[6][7][8] Why are we here? What are we here for?[9][10][11] What is the origin of life?[12] What is the nature of life? What is the nature of reality?[12][13][14] What is the purpose of life? \n",
       "\n",
       "to the meaning of life is too profound to be known and understood.[189] You will never live if you are looking for the meaning of life.[161] The meaning of life is to forget about the search for the meaning of life.[161] Ultimately, a person should not ask what the meaning of their life is, but rather must recognize that it is they themselves who are asked. In a word, each person is questioned by life; and they can only answer to life by answering for their own life; to life they can only respon\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is the meaning of life?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > The meaning of life is a profound question that has been expressed in various ways, but the text suggests that it may be too profound to be fully understood or known. Some perspectives suggest that the search for the meaning of life may be a distraction from actually living it, and that individuals must answer for their own lives in order to find purpose. Therefore, there is no one definitive answer to the question of the meaning of life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"The meaning of life is a profound question that has been expressed in various ways, but the text suggests that it may be too profound to be fully understood or known. Some perspectives suggest that the search for the meaning of life may be a distraction from actually living it, and that individuals must answer for their own lives in order to find purpose. Therefore, there is no one definitive answer to the question of the meaning of life.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > The meaning of life is complex and subjective, with no definitive answer, and may distract from actually living it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LangChain Document Loader is a versatile tool that loads text from various sources and transforms data for language models.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain(\"What is a langchain document loader?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0048\n",
      "Tokens: 16,495\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "f LangChain to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases.What is a LangChain Agent?A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses. They are especially useful when there's an unknown chain of interactions that de\n",
       "\n",
       " LangChain enables chains to interact with external data sources to gather data for the generation step. For example, it can help with summarizing long texts or answering questions using specific data sources.Agents: An agent lets an LLM make decisions about actions, take those actions, check the results, and keep going until the job's done. LangChain provides a standard interface for agents, a variety of agents to choose from, and examples of end-to-end agents.Memory: LangChain has a standard i\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain agent?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent drives decision-making, accesses tools, and builds adaptive applications with context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "LangChain Indexes: Document Loaders                                                                Home About Contact      Sign in Subscribe           LangChain     Featured  LangChain Indexes: Document Loaders Dive into the world of LangChain Document Loaders, understand how they work to transform and load text from various sources and learn how to use them in your language modeling tasks.           David Gentile  May 25, 2023 • 7 min read          Welcome to the LangChain introduction series. \n",
       "\n",
       "es. They are versatile tools that can handle various data formats and transform them into a standard structure that language models can easily process.This guide aims to explain LangChain Document Loaders in-depth, enabling you to make the most of them in your LLM applications.Understanding LangChain Document LoadersThe first concept to understand is what Langchain calls a Document. It really does not get more straightforward as a Document has two fields:page_content (string): the raw text of th\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain document loader?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Document Loader is a versatile tool that can handle various data formats and transform them into a standard structure that language models can easily process. It loads text from various sources and is used in language modeling tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Document Loader is a versatile tool that can handle various data formats and transform them into a standard structure that language models can easily process. It loads text from various sources and is used in language modeling tasks.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Document Loader is a versatile tool that loads text from various sources and transforms data for language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo', temperature=0)\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the last MessageMetaData via: \n",
    "print(f\"MessageMetaData: {chat.previous_message}\")\n",
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `OpenAIChat` model has a `memory_strategy` parameter and takes a `MemoryBuffer` class. A `MemoryBuffer` class is a callable that takes a `list[MessageMetaData]` (i.e. from the `model.history` property) and also returns a `list[MessageMetaData]` serving as the model's memory (i.e. a list containing the messages that will be sent to the model along with the new prompt). This allows the end user to easily define a memory strategy of their own (e.g. keep the first message and the last `n` messages).\n",
    "\n",
    "One Example of a `MemoryBuffer` is a `MemoryBufferMessageWindow` class where you can specify the last `n` messages that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.memory import MemoryBufferMessageWindow\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    memory_strategy=MemoryBufferMessageWindow(last_n_messages=0),  # no memory\n",
    ")\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have access to the full history, but the ChatGPT didn't use any of it.\n",
    "chat._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta(chunk):\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'content' in delta:\n",
    "        return delta['content']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object EngineAPIResource.create.<locals>.<genexpr> at 0xffff8044bcd0>\n",
      "None\n",
      "As\n",
      " an\n",
      " AI\n",
      " language model, I do not have personal beliefs or opinions, but the meaning of life is subjective and varies from person to person."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# Example of an OpenAI ChatCompletion request with stream=True\n",
    "# https://platform.openai.com/docs/guides/chat\n",
    "\n",
    "# a ChatCompletion request\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"What is the meaning of life? Answer in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "print(response)\n",
    "print(get_delta(next(response)))\n",
    "message = next(response)\n",
    "print(get_delta(message))\n",
    "print(get_delta(next(response)))\n",
    "print(get_delta(next(response)))\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'content' in delta:\n",
    "        print(delta['content'], end='')\n",
    "    # print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion.chunk id=chatcmpl-7SFy2ytujikSVaaIuW4JSOeKWWAT2 at 0xffff8042b950> JSON: {\n",
       "  \"id\": \"chatcmpl-7SFy2ytujikSVaaIuW4JSOeKWWAT2\",\n",
       "  \"object\": \"chat.completion.chunk\",\n",
       "  \"created\": 1686968918,\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"delta\": {\n",
       "        \"content\": \"As\"\n",
       "      },\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": null\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      " large\n",
      " language\n",
      " model\n",
      " is\n",
      " a\n",
      " type\n",
      " of\n",
      " artificial\n",
      " intelligence\n",
      " that\n",
      " uses\n",
      " deep\n",
      " learning\n",
      " to\n",
      " generate\n",
      " human\n",
      "-like\n",
      " language\n",
      " and\n",
      " understand\n",
      " natural\n",
      " language\n",
      " processing\n",
      " tasks\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    streaming_callback=lambda x: print(x.response),\n",
    "    )\n",
    "response = chat(\"Explain what a large language model is in a single sentence.\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000083\n",
      "    Total Tokens: 47\n",
      "    Total Prompt Tokens: 21\n",
      "    Total Response Tokens: 26\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)\n",
    "\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence that uses deep learning to generate human-like language and understand natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence that uses deep learning to generate human-like language and understand natural language processing tasks.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    # streaming_callback=lambda x: print(x.response, end=''),\n",
    "    )\n",
    "response = chat(\"Explain what a large language model is in a single sentence.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000100\n",
      "    Total Tokens: 58\n",
      "    Total Prompt Tokens: 32\n",
      "    Total Response Tokens: 26\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Hello\"\n",
    "\n",
    "def update_string():\n",
    "    global my_string  # Declare the variable as global\n",
    "    my_string += \" World\"  # Update the string by appending \" World\"\n",
    "\n",
    "# Before calling the function\n",
    "print(my_string)  # Output: Hello\n",
    "\n",
    "# Call the function to update the string\n",
    "update_string()\n",
    "\n",
    "# After calling the function\n",
    "print(my_string)  # Output: Hello World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624322322349568"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a random number generator\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Generate a random list of floats between 0.5 and 13.3\n",
    "random_floats = rng.uniform(low=-2, high=2, size=50)\n",
    "random_floats.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
