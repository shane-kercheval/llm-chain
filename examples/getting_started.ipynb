{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| meaning| of| life| is| a| philosophical| question| that| has| been| debated| by| scholars|,| theolog|ians|,| and| philosophers| for| centuries|.| There| is| no| one| definitive| answer| to| this| question|,| as| it| can| vary| depending| on| one|'s| beliefs|,| values|,| and| experiences|.| Some| people| believe| that| the| meaning| of| life| is| to| seek| happiness|,| while| others| believe| it| is| to| fulfill| a| specific| purpose| or| destiny|.| Ultimately|,| the| meaning| of| life| is| a| personal| and| subjective| concept| that| each| individual| must| determine| for| themselves|.|"
     ]
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    streaming_callback=lambda x: print(x.response, end='|')\n",
    ")\n",
    "response = chat(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The meaning of life is a philosophical question that has been debated by '\n",
      " 'scholars, theologians, and philosophers for centuries. There is no one '\n",
      " \"definitive answer to this question, as it can vary depending on one's \"\n",
      " 'beliefs, values, and experiences. Some people believe that the meaning of '\n",
      " 'life is to seek happiness, while others believe it is to fulfill a specific '\n",
      " 'purpose or destiny. Ultimately, the meaning of life is a personal and '\n",
      " 'subjective concept that each individual must determine for themselves.')\n"
     ]
    }
   ],
   "source": [
    "# response is streamed above but also returned at the end of the call\n",
    "from pprint import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000227\n",
      "    Total Tokens: 120\n",
      "    Total Prompt Tokens: 26\n",
      "    Total Response Tokens: 94\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous prompt: What is the meaning of life?\n",
      "previous response: The meaning of life is a philosophical question that has been debated by scholars, theologians, and philosophers for centuries. There is no one definitive answer to this question, as it can vary depending on one's beliefs, values, and experiences. Some people believe that the meaning of life is to seek happiness, while others believe it is to fulfill a specific purpose or destiny. Ultimately, the meaning of life is a personal and subjective concept that each individual must determine for themselves.\n"
     ]
    }
   ],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-19 00:40:13.138; prompt: \"What is the meaning ...\"; response: \"The meaning of life ...\";  cost: $0.000227; total_tokens: 120; metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'What is the meaning of life?'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Web Search (via DuckDuckGo) to ask questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| meaning| of| life| is| complex| and| subjective|,| with| no| definitive| answer|,| and| may| distract| from| actually| living| it|.|"
     ]
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import duckduckgo_search, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain, Value\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "streaming_callback = lambda x: print(x.response, end='|')\n",
    "streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo', streaming_callback=streaming_callback)\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "initial_question = Value()\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    initial_question,\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    initial_question,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "\n",
    "response = chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is complex and subjective, with no definitive answer, and may distract from actually living it.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0053\n",
      "Tokens: 45,535\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "the meaning of life at all? To what purpose is it played, this farce in which everything that is essential is irrevocably fixed and determined?[5]Questions about the meaning of life have been expressed in a broad variety of other ways, including: What is the meaning of life? What's it all about? Who are we?[6][7][8] Why are we here? What are we here for?[9][10][11] What is the origin of life?[12] What is the nature of life? What is the nature of reality?[12][13][14] What is the purpose of life? \n",
       "\n",
       "to the meaning of life is too profound to be known and understood.[189] You will never live if you are looking for the meaning of life.[161] The meaning of life is to forget about the search for the meaning of life.[161] Ultimately, a person should not ask what the meaning of their life is, but rather must recognize that it is they themselves who are asked. In a word, each person is questioned by life; and they can only answer to life by answering for their own life; to life they can only respon\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is the meaning of life?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > The meaning of life is a profound question that has been expressed in various ways, but the text suggests that it may be too profound to be fully understood or known. Some perspectives suggest that the search for the meaning of life may be a distraction from actually living it, and that individuals must answer for their own lives in order to find purpose. Therefore, there is no one definitive answer to the question of the meaning of life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"The meaning of life is a profound question that has been expressed in various ways, but the text suggests that it may be too profound to be fully understood or known. Some perspectives suggest that the search for the meaning of life may be a distraction from actually living it, and that individuals must answer for their own lives in order to find purpose. Therefore, there is no one definitive answer to the question of the meaning of life.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > The meaning of life is complex and subjective, with no definitive answer, and may distract from actually living it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LangChain Document Loader is a versatile tool that loads text from various sources and transforms data for language models.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain(\"What is a langchain document loader?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0048\n",
      "Tokens: 16,495\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "f LangChain to build advanced language model applications that are adaptable, efficient, and capable of handling complex use cases.What is a LangChain Agent?A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses. They are especially useful when there's an unknown chain of interactions that de\n",
       "\n",
       " LangChain enables chains to interact with external data sources to gather data for the generation step. For example, it can help with summarizing long texts or answering questions using specific data sources.Agents: An agent lets an LLM make decisions about actions, take those actions, check the results, and keep going until the job's done. LangChain provides a standard interface for agents, a variety of agents to choose from, and examples of end-to-end agents.Memory: LangChain has a standard i\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain agent?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Agent is an entity that drives decision-making in the framework. It has access to a set of tools and can decide which tool to call based on the user's input. Agents help build complex applications that require adaptive and context-specific responses.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Agent drives decision-making, accesses tools, and builds adaptive applications with context-specific responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "LangChain Indexes: Document Loaders                                                                Home About Contact      Sign in Subscribe           LangChain     Featured  LangChain Indexes: Document Loaders Dive into the world of LangChain Document Loaders, understand how they work to transform and load text from various sources and learn how to use them in your language modeling tasks.           David Gentile  May 25, 2023 • 7 min read          Welcome to the LangChain introduction series. \n",
       "\n",
       "es. They are versatile tools that can handle various data formats and transform them into a standard structure that language models can easily process.This guide aims to explain LangChain Document Loaders in-depth, enabling you to make the most of them in your LLM applications.Understanding LangChain Document LoadersThe first concept to understand is what Langchain calls a Document. It really does not get more straightforward as a Document has two fields:page_content (string): the raw text of th\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "What is a langchain document loader?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Document Loader is a versatile tool that can handle various data formats and transform them into a standard structure that language models can easily process. It loads text from various sources and is used in language modeling tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## MESSAGE:  3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### PROMPT:\n",
       "Summarize the following in less than 20 words: \"A LangChain Document Loader is a versatile tool that can handle various data formats and transform them into a standard structure that language models can easily process. It loads text from various sources and is used in language modeling tasks.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RESPONSE:\n",
       " > A LangChain Document Loader is a versatile tool that loads text from various sources and transforms data for language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "for index, record in enumerate(chain.message_history):\n",
    "    display(Markdown(f\"## MESSAGE:  {index}\"))\n",
    "    display(Markdown(f\"### PROMPT:\\n{record.prompt.strip()}\"))\n",
    "    display(Markdown(f\"### RESPONSE:\\n > {record.response.strip()}\"))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo', temperature=0)\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the last MessageMetaData via: \n",
    "print(f\"MessageMetaData: {chat.previous_message}\")\n",
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `OpenAIChat` model has a `memory_strategy` parameter and takes a `MemoryBuffer` class. A `MemoryBuffer` class is a callable that takes a `list[MessageMetaData]` (i.e. from the `model.history` property) and also returns a `list[MessageMetaData]` serving as the model's memory (i.e. a list containing the messages that will be sent to the model along with the new prompt). This allows the end user to easily define a memory strategy of their own (e.g. keep the first message and the last `n` messages).\n",
    "\n",
    "One Example of a `MemoryBuffer` is a `MemoryBufferMessageWindow` class where you can specify the last `n` messages that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.memory import MemoryBufferMessageWindow\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    memory_strategy=MemoryBufferMessageWindow(last_n_messages=0),  # no memory\n",
    ")\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have access to the full history, but the ChatGPT didn't use any of it.\n",
    "chat._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta(chunk):\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'content' in delta:\n",
    "        return delta['content']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object EngineAPIResource.create.<locals>.<genexpr> at 0xffff8044bcd0>\n",
      "None\n",
      "As\n",
      " an\n",
      " AI\n",
      " language model, I do not have personal beliefs or opinions, but the meaning of life is subjective and varies from person to person."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# Example of an OpenAI ChatCompletion request with stream=True\n",
    "# https://platform.openai.com/docs/guides/chat\n",
    "\n",
    "# a ChatCompletion request\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"What is the meaning of life? Answer in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "print(response)\n",
    "print(get_delta(next(response)))\n",
    "message = next(response)\n",
    "print(get_delta(message))\n",
    "print(get_delta(next(response)))\n",
    "print(get_delta(next(response)))\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'content' in delta:\n",
    "        print(delta['content'], end='')\n",
    "    # print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion.chunk id=chatcmpl-7SFy2ytujikSVaaIuW4JSOeKWWAT2 at 0xffff8042b950> JSON: {\n",
       "  \"id\": \"chatcmpl-7SFy2ytujikSVaaIuW4JSOeKWWAT2\",\n",
       "  \"object\": \"chat.completion.chunk\",\n",
       "  \"created\": 1686968918,\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"delta\": {\n",
       "        \"content\": \"As\"\n",
       "      },\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": null\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      " large\n",
      " language\n",
      " model\n",
      " is\n",
      " a\n",
      " type\n",
      " of\n",
      " artificial\n",
      " intelligence\n",
      " that\n",
      " uses\n",
      " deep\n",
      " learning\n",
      " to\n",
      " generate\n",
      " human\n",
      "-like\n",
      " language\n",
      " and\n",
      " understand\n",
      " natural\n",
      " language\n",
      " processing\n",
      " tasks\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    streaming_callback=lambda x: print(x.response),\n",
    "    )\n",
    "response = chat(\"Explain what a large language model is in a single sentence.\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000083\n",
      "    Total Tokens: 47\n",
      "    Total Prompt Tokens: 21\n",
      "    Total Response Tokens: 26\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)\n",
    "\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence that uses deep learning to generate human-like language and understand natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence that uses deep learning to generate human-like language and understand natural language processing tasks.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    # streaming_callback=lambda x: print(x.response, end=''),\n",
    "    )\n",
    "response = chat(\"Explain what a large language model is in a single sentence.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000100\n",
      "    Total Tokens: 58\n",
      "    Total Prompt Tokens: 32\n",
      "    Total Response Tokens: 26\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Hello\"\n",
    "\n",
    "def update_string():\n",
    "    global my_string  # Declare the variable as global\n",
    "    my_string += \" World\"  # Update the string by appending \" World\"\n",
    "\n",
    "# Before calling the function\n",
    "print(my_string)  # Output: Hello\n",
    "\n",
    "# Call the function to update the string\n",
    "update_string()\n",
    "\n",
    "# After calling the function\n",
    "print(my_string)  # Output: Hello World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624322322349568"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a random number generator\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Generate a random list of floats between 0.5 and 13.3\n",
    "random_floats = rng.uniform(low=-2, high=2, size=50)\n",
    "random_floats.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7SunYoE7pHohuSwMNp5ZzwUkpx288 at 0xffff3c2a29f0> JSON: {\n",
       "  \"id\": \"cmpl-7SunYoE7pHohuSwMNp5ZzwUkpx288\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1687125872,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"there lived a gorgeous princess named Aurora. She had long, glowing golden hair and\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 16,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tenacity\n",
    "from typing import Callable\n",
    "\n",
    "def retry_handler(num_retries: int = 3, wait_fixed: int = 1) -> Callable:\n",
    "    return tenacity.Retrying(\n",
    "        stop=tenacity.stop_after_attempt(num_retries),\n",
    "        wait=tenacity.wait_fixed(wait_fixed),\n",
    "        reraise=True\n",
    "    )\n",
    "\n",
    "r = retry_handler()\n",
    "r(\n",
    "    openai.Completion.create,\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Once upon a time,\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7StDBWFEuPyzS3fBPB1JzuO6HHI6O at 0xffff4689a2d0> JSON: {\n",
       "  \"id\": \"cmpl-7StDBWFEuPyzS3fBPB1JzuO6HHI6O\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1687119773,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" there was a young man named Jacob who lived alone in a small town in the\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 16,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "completion_with_backoff(model=\"text-davinci-003\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
