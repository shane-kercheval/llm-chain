{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set above. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find relevant documents to include in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3042077414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'This is a document. It has information related to the question I want to ask.',\n",
    "    'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "    'Here is another document.',\n",
    "]\n",
    "question = \"What is the answer for the codeword `flibberwump`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.base import Document\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")\n",
    "document_index.add_documents(docs=[Document(content=x) for x in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer for the codeword `flibberwump` is `hanzo`.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "doc_template = DocSearchTemplate(doc_index=document_index, n_docs=1)\n",
    "\n",
    "# A chain is simply a collection of callables where the output of the previous callable matches\n",
    "# the input to the next callable.\n",
    "# Below, the input to the `DocSearchTemplate` is a string (the question) and the output is a\n",
    "# string (the prompt); and the input to `OpenAIChat` is a string (the prompt).\n",
    "# Question (str) -> Prompt (str) -> Answer (str)\n",
    "chain = Chain(links=[\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 171\n",
      "Cost: $0.0002492\n"
     ]
    }
   ],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:01:34; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 41; cost: $0.000016\n",
      "timestamp: 2023-06-13 02:01:34; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 17; cost: $0.000007\n",
      "timestamp: 2023-06-13 02:01:36; prompt: \"Answer the question ...\"; response: \"The answer for the c...\";  total_tokens: 113; cost: $0.000226 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# you can see the individual costs for the embeddings and the chat\n",
    "# The embeddings model has 2 records in its history; \n",
    "# 1 to embed the original docs and the other to embed the question passed into the chain\n",
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
      "\n",
      "Here is the information:\n",
      "\n",
      "```\n",
      "The codeword is `flibberwump`; the answer is `hanzo`.\n",
      "```\n",
      "\n",
      "Here is the question:\n",
      "\n",
      "What is the answer for the codeword `flibberwump`?\n",
      "\n",
      "response: The answer for the codeword `flibberwump` is `hanzo`.\n",
      "cost: 0.000226\n"
     ]
    }
   ],
   "source": [
    "# The chat has model has 1 record in its history\n",
    "chat_model = chain[1]\n",
    "print(f\"prompt: {chat_model._history[0].prompt}\")\n",
    "print(f\"response: {chat_model._history[0].response}\")\n",
    "print(f\"cost: {chat_model._history[0].cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL -> doc -> text-splitter -> list[docs] -> vector-db (embeddings) -> None\n",
    "\n",
    "query -> search -> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.models import OpenAIEmbeddings\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# create a document index (i.e. vector database) and add the text from above.\n",
    "document_index = ChromaDocumentIndex(\n",
    "    embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text():\n",
    "    return [\n",
    "        'This is a document. It has information related to the question I want to ask.',\n",
    "        'The codeword is `flibberwump`; the answer is `hanzo`.',\n",
    "        'Here is another document.',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer for the codeword `flibberwump` is `hanzo`.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <nothing> -> list[str]\n",
    "# list[str] -> None\n",
    "# <ignored> -> str\n",
    "# str -> str\n",
    "# str -> str\n",
    "chain = Chain(links=[\n",
    "    load_text,\n",
    "    lambda texts: document_index.add_documents(docs=[Document(content=x) for x in texts]),\n",
    "    lambda _: \"What is the answer for the codeword `flibberwump`?\",\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=1),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "response = chain()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 171\n",
      "Cost: $0.0002492\n"
     ]
    }
   ],
   "source": [
    "# the chain tracks the usage across any object that has `total_tokens` and `total_cost` properties\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")\n",
    "print(f\"Cost: ${chain.total_cost:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:23:09; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 41; cost: $0.000016\n",
      "timestamp: 2023-06-13 02:23:09; metadata: {'model_name': 'text-embedding-ada-002'} total_tokens: 17; cost: $0.000007\n",
      "timestamp: 2023-06-13 02:23:11; prompt: \"Answer the question ...\"; response: \"The answer for the c...\";  total_tokens: 113; cost: $0.000226 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agents — 🦜🔗 LangChain 0.0.198\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n🦜🔗 LangChain 0.0.198\\n\\n\\n\\nGetting Started\\n\\nQuickstart Guide\\nConcepts\\nTutorials\\n\\nModules\\n\\nModels\\nGetting Started\\nLLMs\\nGetting Started\\nGeneric Functionality\\nHow to use the async API for LLMs\\nHow to write a custom LLM wrapper\\nHow (and why) to use the fake LLM\\nHow (and why) to use the human input LLM\\nHow to cache LLM calls\\nHow to serialize LLM classes\\nHow to stream LLM and Chat Model responses\\nHow to track token usage\\n\\n\\nIntegrations\\nAI21\\nAleph Alpha\\nAnyscale\\nAviary\\nAzure OpenAI\\nBanana\\nBaseten\\nBeam\\nBedrock\\nCerebriumAI\\nCohere\\nC Transformers\\nDatabricks\\nDeepInfra\\nForefrontAI\\nGoogle Cloud Platform Vertex AI PaLM\\nGooseAI\\nGPT4All\\nHugging Face Hub\\nHugging Face Pipeline\\nHuggingface TextGen Inference\\nJsonformer\\nLlama-cpp\\nManifest\\nModal\\nMosaicML\\nNLP Cloud\\nOpenAI\\nOpenLM\\nPetals\\nPipelineAI\\nPrediction Guard\\nPromptLayer OpenAI\\nReLLM\\nReplicate\\nRunhouse\\nSageMaker Endpoint\\nStochasticAI\\nWriter\\n\\n\\nReference\\n\\n\\nChat Models\\nGetting Started\\nHow-To Guides\\nHow to use few shot examples\\nHow to stream responses\\n\\n\\nIntegrations\\nAnthropic\\nAzure\\nGoogle Vertex AI PaLM\\nOpenAI\\nPromptLayer ChatOpenAI\\n\\n\\n\\n\\nText Embedding Models\\nAleph Alpha\\nAmazon Bedrock\\nAzure OpenAI\\nCohere\\nDashScope\\nDeepInfra\\nElasticsearch\\nEmbaas\\nFake Embeddings\\nGoogle Vertex AI PaLM\\nHugging Face Hub\\nHuggingFace Instruct\\nJina\\nLlama-cpp\\nMiniMax\\nModelScope\\nMosaicML\\nOpenAI\\nSageMaker Endpoint\\nSelf Hosted Embeddings\\nSentence Transformers\\nTensorflow Hub\\n\\n\\n\\n\\nPrompts\\nGetting Started\\nPrompt Templates\\nGetting Started\\nHow-To Guides\\nConnecting to a Feature Store\\nHow to create a custom prompt template\\nHow to create a prompt template that uses few shot examples\\nHow to work with partial Prompt Templates\\nPrompt Composition\\nHow to serialize prompts\\n\\n\\nReference\\nPromptTemplates\\nExample Selector\\nOutput Parsers\\n\\n\\n\\n\\nChat Prompt Templates\\nExample Selectors\\nHow to create a custom example selector\\nLengthBased ExampleSelector\\nMaximal Marginal Relevance ExampleSelector\\nNGram Overlap ExampleSelector\\nSimilarity ExampleSelector\\n\\n\\nOutput Parsers\\nOutput Parsers\\nCommaSeparatedListOutputParser\\nDatetime\\nEnum Output Parser\\nOutputFixingParser\\nPydanticOutputParser\\nRetryOutputParser\\nStructured Output Parser\\n\\n\\n\\n\\nMemory\\nGetting Started\\nHow-To Guides\\nConversationBufferMemory\\nConversationBufferWindowMemory\\nEntity Memory\\nConversation Knowledge Graph Memory\\nConversationSummaryMemory\\nConversationSummaryBufferMemory\\nConversationTokenBufferMemory\\nVectorStore-Backed Memory\\nHow to add Memory to an LLMChain\\nHow to add memory to a Multi-Input Chain\\nHow to add Memory to an Agent\\nAdding Message Memory backed by a database to an Agent\\nCassandra Chat Message History\\nHow to customize conversational memory\\nHow to create a custom Memory class\\nDynamodb Chat Message History\\nEntity Memory with SQLite storage\\nMomento Chat Message History\\nMongodb Chat Message History\\nMotörhead Memory\\nMotörhead Memory (Managed)\\nHow to use multiple memory classes in the same chain\\nPostgres Chat Message History\\nRedis Chat Message History\\nZep Memory\\n\\n\\n\\n\\nIndexes\\nGetting Started\\nDocument Loaders\\nAirtable\\nOpenAIWhisperParser\\nCoNLL-U\\nCopy Paste\\nCSV\\nEmail\\nEPub\\nEverNote\\nMicrosoft Excel\\nFacebook Chat\\nFile Directory\\nHTML\\nImages\\nJupyter Notebook\\nJSON\\nMarkdown\\nMicrosoft PowerPoint\\nMicrosoft Word\\nOpen Document Format (ODT)\\nPandas DataFrame\\nPDF\\nSitemap\\nSubtitle\\nTelegram\\nTOML\\nUnstructured File\\nURL\\nWebBaseLoader\\nWeather\\nWhatsApp Chat\\nArxiv\\nAZLyrics\\nBiliBili\\nCollege Confidential\\nGutenberg\\nHacker News\\nHuggingFace dataset\\niFixit\\nIMSDb\\nMediaWikiDump\\nWikipedia\\nYouTube transcripts\\nAirbyte JSON\\nApify Dataset\\nAWS S3 Directory\\nAWS S3 File\\nAzure Blob Storage Container\\nAzure Blob Storage File\\nBlackboard\\nBlockchain\\nChatGPT Data\\nConfluence\\nDiffbot\\nDocugami\\nDuckDB\\nFauna\\nFigma\\nGitBook\\nGit\\nGoogle BigQuery\\nGoogle Cloud Storage Directory\\nGoogle Cloud Storage File\\nGoogle Drive\\nImage captions\\nIugu\\nJoplin\\nMicrosoft OneDrive\\nModern Treasury\\nNotion DB 2/2\\nNotion DB 1/2\\nObsidian\\nPsychic\\nPySpark DataFrame Loader\\nReadTheDocs Documentation\\nReddit\\nRoam\\nSlack\\nSnowflake\\nSpreedly\\nStripe\\n2Markdown\\nTwitter\\n\\n\\nText Splitters\\nGetting Started\\nCharacter\\nCodeTextSplitter\\nNLTK\\nRecursive Character\\nspaCy\\nTiktoken\\nHugging Face tokenizer\\ntiktoken (OpenAI) tokenizer\\n\\n\\nVectorstores\\nGetting Started\\nAnalyticDB\\nAnnoy\\nAtlas\\nAwaDB\\nAzure Cognitive Search\\nChroma\\nClickHouse Vector Search\\nDeep Lake\\nDocArrayHnswSearch\\nDocArrayInMemorySearch\\nElasticSearch\\nFAISS\\nHologres\\nLanceDB\\nMatchingEngine\\nMilvus\\nMyScale\\nOpenSearch\\nPGVector\\nPinecone\\nQdrant\\nRedis\\nSingleStoreDB vector search\\nSKLearnVectorStore\\nSupabase (Postgres)\\nTair\\nTigris\\nTypesense\\nVectara\\nWeaviate\\nZilliz\\n\\n\\nRetrievers\\nArxiv\\nAWS Kendra\\nAzure Cognitive Search\\nChatGPT Plugin\\nSelf-querying with Chroma\\nCohere Reranker\\nContextual Compression\\nDataberry\\nElasticSearch BM25\\nkNN\\nLOTR (Merger Retriever)\\nMetal\\nPinecone Hybrid Search\\nPubMed Retriever\\nSelf-querying with Qdrant\\nSelf-querying\\nSVM\\nTF-IDF\\nTime Weighted VectorStore\\nVectorStore\\nVespa\\nWeaviate Hybrid Search\\nSelf-querying with Weaviate\\nWikipedia\\nZep\\n\\n\\n\\n\\nChains\\nGetting Started\\nHow-To Guides\\nAsync API for Chain\\nCreating a custom Chain\\nLoading from LangChainHub\\nLLM Chain\\nRouter Chains\\nSequential Chains\\nSerialization\\nTransformation Chain\\nAnalyze Document\\nChat Over Documents with Chat History\\nGraph QA\\nHypothetical Document Embeddings\\nQuestion Answering with Sources\\nQuestion Answering\\nSummarization\\nRetrieval Question/Answering\\nRetrieval Question Answering with Sources\\nVector DB Text Generation\\nAPI Chains\\nSelf-Critique Chain with Constitutional AI\\nFLARE\\nGraphCypherQAChain\\nNebulaGraphQAChain\\nBashChain\\nLLMCheckerChain\\nLLM Math\\nLLMRequestsChain\\nLLMSummarizationCheckerChain\\nModeration\\nRouter Chains: Selecting from multiple prompts with MultiPromptChain\\nRouter Chains: Selecting from multiple prompts with MultiRetrievalQAChain\\nOpenAPI Chain\\nPAL\\nSQL Chain example\\n\\n\\nReference\\n\\n\\nAgents\\nGetting Started\\nTools\\nGetting Started\\nDefining Custom Tools\\nMulti-Input Tools\\nTool Input Schema\\nApify\\nArXiv API Tool\\nAWS Lambda API\\nShell Tool\\nBing Search\\nBrave Search\\nChatGPT Plugins\\nDuckDuckGo Search\\nFile System Tools\\nGoogle Places\\nGoogle Search\\nGoogle Serper API\\nGradio Tools\\nGraphQL tool\\nHuggingFace Tools\\nHuman as a tool\\nIFTTT WebHooks\\nMetaphor Search\\nOpenWeatherMap API\\nPubMed Tool\\nPython REPL\\nRequests\\nSceneXplain\\nSearch Tools\\nSearxNG Search API\\nSerpAPI\\nTwilio\\nWikipedia\\nWolfram Alpha\\nYouTubeSearchTool\\nZapier Natural Language Actions API\\n\\n\\nAgents\\nAgent Types\\nCustom Agent\\nCustom LLM Agent\\nCustom LLM Agent (with a ChatModel)\\nCustom MRKL Agent\\nCustom MultiAction Agent\\nCustom Agent with Tool Retrieval\\nConversation Agent (for Chat Models)\\nConversation Agent\\nMRKL\\nMRKL Chat\\nReAct\\nSelf Ask With Search\\nStructured Tool Chat Agent\\n\\n\\nToolkits\\nAzure Cognitive Services Toolkit\\nCSV Agent\\nGmail Toolkit\\nJira\\nJSON Agent\\nOpenAPI agents\\nNatural Language APIs\\nPandas Dataframe Agent\\nPlayWright Browser Toolkit\\nPowerBI Dataset Agent\\nPython Agent\\nSpark Dataframe Agent\\nSpark SQL Agent\\nSQL Database Agent\\nVectorstore Agent\\n\\n\\nAgent Executors\\nHow to combine agents and vectorstores\\nHow to use the async API for Agents\\nHow to create ChatGPT Clone\\nHandle Parsing Errors\\nHow to access intermediate steps\\nHow to cap the max number of iterations\\nHow to use a timeout for the agent\\nHow to add SharedMemory to an Agent and its Tools\\n\\n\\nPlan and Execute\\n\\n\\nCallbacks\\n\\nUse Cases\\n\\nAutonomous Agents\\nAgent Simulations\\nAgents\\nQuestion Answering over Docs\\nChatbots\\nQuerying Tabular Data\\nCode Understanding\\nInteracting with APIs\\nExtraction\\nSummarization\\nEvaluation\\nAgent Benchmarking: Search + Calculator\\nAgent VectorDB Question Answering Benchmarking\\nBenchmarking Template\\nData Augmented Question Answering\\nGeneric Agent Evaluation\\nUsing Hugging Face Datasets\\nLLM Math\\nEvaluating an OpenAPI Chain\\nQuestion Answering Benchmarking: Paul Graham Essay\\nQuestion Answering Benchmarking: State of the Union Address\\nQA Generation\\nQuestion Answering\\nSQL Question Answering Benchmarking: Chinook\\n\\n\\n\\nReference\\n\\nInstallation\\nAPI References\\nModels\\nLLMs\\nChat Models\\nEmbeddings\\n\\n\\nPrompts\\nPromptTemplates\\nExample Selector\\nOutput Parsers\\n\\n\\nIndexes\\nDocstore\\nText Splitter\\nDocument Loaders\\nVector Stores\\nRetrievers\\nDocument Compressors\\nDocument Transformers\\n\\n\\nMemory\\nChains\\nAgents\\nAgents\\nTools\\nAgent Toolkits\\n\\n\\nUtilities\\nExperimental Modules\\n\\n\\n\\nEcosystem\\n\\nIntegrations\\nTracing Walkthrough\\nAI21 Labs\\nAim\\nAirbyte\\nAleph Alpha\\nAmazon Bedrock\\nAnalyticDB\\nAnnoy\\nAnthropic\\nAnyscale\\nApify\\nArgilla\\nArxiv\\nAtlasDB\\nAwaDB\\nAWS S3 Directory\\nAZLyrics\\nAzure Blob Storage\\nAzure Cognitive Search\\nAzure OpenAI\\nBanana\\nBeam\\nBiliBili\\nBlackboard\\nCassandra\\nCerebriumAI\\nChroma\\nClearML\\nClickHouse\\nCohere\\nCollege Confidential\\nComet\\nConfluence\\nC Transformers\\nDataberry\\nDatabricks\\nDeepInfra\\nDeep Lake\\nDiffbot\\nDiscord\\nDocugami\\nDuckDB\\nElasticsearch\\nEverNote\\nFacebook Chat\\nFigma\\nForefrontAI\\nGit\\nGitBook\\nGoogle BigQuery\\nGoogle Cloud Storage\\nGoogle Drive\\nGoogle Search\\nGoogle Serper\\nGoogle Vertex AI\\nGooseAI\\nGPT4All\\nGraphsignal\\nGutenberg\\nHacker News\\nHazy Research\\nHelicone\\nHugging Face\\niFixit\\nIMSDb\\nJina\\nLanceDB\\nLangChain Decorators ✨\\nLlama.cpp\\nMediaWikiDump\\nMetal\\nMicrosoft OneDrive\\nMicrosoft PowerPoint\\nMicrosoft Word\\nMilvus\\nMLflow\\nModal\\nModern Treasury\\nMomento\\nMyScale\\nNLPCloud\\nNotion DB\\nObsidian\\nOpenAI\\nOpenSearch\\nOpenWeatherMap\\nPetals\\nPGVector\\nPinecone\\nPipelineAI\\nPrediction Guard\\nPromptLayer\\nPsychic\\nQdrant\\nRay Serve\\nRebuff\\nReddit\\nRedis\\nReplicate\\nRoam\\nRunhouse\\nRWKV-4\\nSageMaker Endpoint\\nSearxNG Search API\\nSerpAPI\\nShale Protocol\\nscikit-learn\\nSlack\\nspaCy\\nSpreedly\\nStochasticAI\\nStripe\\nTair\\nTelegram\\nTensorflow Hub\\n2Markdown\\nTrello\\nTwitter\\nUnstructured\\nVectara\\nVespa\\nWeights & Biases\\nWeather\\nWeaviate\\nWhatsApp\\nWhyLabs\\nWikipedia\\nWolfram Alpha\\nWriter\\nYeager.ai\\nYouTube\\nZep\\nZilliz\\n\\n\\nDependents\\nDeployments\\n\\nAdditional Resources\\n\\nLangChainHub\\nDeploying LLMs in Production\\nGallery\\nTracing\\nModel Comparison\\nDiscord\\nYouTube\\nProduction Support\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n.md\\n\\n\\n\\n\\n\\n\\n\\n.pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAgents\\n\\n\\n\\n\\n Contents \\n\\n\\n\\nCreate Your Own Agent\\nStep 1: Create Tools\\n(Optional) Step 2: Modify Agent\\n(Optional) Step 3: Modify Agent Executor\\n\\n\\nExamples\\n\\n\\n\\n\\n\\n\\n\\n\\nAgents#\\n\\nConceptual Guide\\n\\nAgents can be used for a variety of tasks.\\nAgents combine the decision making ability of a language model with tools in order to create a system\\nthat can execute and implement solutions on your behalf. Before reading any more, it is highly\\nrecommended that you read the documentation in the agent module to understand the concepts associated with agents more.\\nSpecifically, you should be familiar with what the agent, tool, and agent executor abstractions are before reading more.\\n\\nAgent Documentation (for interacting with the outside world)\\n\\n\\nCreate Your Own Agent#\\nOnce you have read that documentation, you should be prepared to create your own agent.\\nWhat exactly does that involve?\\nHere’s how we recommend getting started with creating your own agent:\\n\\nStep 1: Create Tools#\\nAgents are largely defined by the tools they can use.\\nIf you have a specific task you want the agent to accomplish, you have to give it access to the right tools.\\nWe have many tools natively in LangChain, so you should first look to see if any of them meet your needs.\\nBut we also make it easy to define a custom tool, so if you need custom tools you should absolutely do that.\\n\\n\\n(Optional) Step 2: Modify Agent#\\nThe built-in LangChain agent types are designed to work well in generic situations,\\nbut you may be able to improve performance by modifying the agent implementation.\\nThere are several ways you could do this:\\n\\nModify the base prompt. This can be used to give the agent more context on how it should behave, etc.\\nModify the output parser. This is necessary if the agent is having trouble parsing the language model output.\\n\\n\\n\\n(Optional) Step 3: Modify Agent Executor#\\nThis step is usually not necessary, as this is pretty general logic.\\nPossible reasons you would want to modify this include adding different stopping conditions, or handling errors\\n\\n\\n\\nExamples#\\nSpecific examples of agents include:\\n\\nAI Plugins: an implementation of an agent that is designed to be able to use all AI Plugins.\\nPlug-and-PlAI (Plugins Database): an implementation of an agent that is designed to be able to use all AI Plugins retrieved from PlugNPlAI.\\nWikibase Agent: an implementation of an agent that is designed to interact with Wikibase.\\nSales GPT: This notebook demonstrates an implementation of a Context-Aware AI Sales agent.\\nMulti-Modal Output Agent: an implementation of a multi-modal output agent that can generate text and images.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\nAgent Simulations\\n\\n\\n\\n\\nnext\\nQuestion Answering over Docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Contents\\n  \\n\\n\\nCreate Your Own Agent\\nStep 1: Create Tools\\n(Optional) Step 2: Modify Agent\\n(Optional) Step 3: Modify Agent Executor\\n\\n\\nExamples\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy Harrison Chase\\n\\n\\n\\n\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n\\n\\n\\n\\n  Last updated on Jun 13, 2023.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "\n",
    "doc = html_page_loader(url='https://python.langchain.com/en/latest/modules/agents.html')\n",
    "doc.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain Agent selects tools based on user input.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.chains import Chain\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.base import Document\n",
    "from llm_chain.chains import Chain\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model_name='text-embedding-ada-002')\n",
    "document_index = ChromaDocumentIndex(embeddings_model=embeddings_model)\n",
    "text_to_docs = lambda x: [Document(content=x.replace('\\n', ' '))]\n",
    "\n",
    "# url to scrape\n",
    "url = 'https://python.langchain.com/en/latest/modules/agents.html'\n",
    "# questions for ChatGPT; each link in the chain must be a callable\n",
    "question_1 = lambda _: \"What is a langchain `Agent`?\"\n",
    "question_2 = lambda x: f'Summarize the following in less than 10 words: \"{x}\"'\n",
    "\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    html_page_loader,\n",
    "    text_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    question_1,\n",
    "    DocSearchTemplate(doc_index=document_index, n_docs=2),\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "    question_2,\n",
    "    OpenAIChat(model_name='gpt-3.5-turbo'),\n",
    "])\n",
    "\n",
    "response = chain(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.0021\n",
      "Tokens: 3,385\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: {chain.total_cost:.4f}\")\n",
    "print(f\"Tokens: {chain.total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 04:37:39; cost: $0.001172; total_tokens: 2,930; metadata: {'model_name': 'text-embedding-ada-002'}\n",
      "timestamp: 2023-06-13 04:37:39; cost: $0.000004; total_tokens: 9; metadata: {'model_name': 'text-embedding-ada-002'}\n",
      "timestamp: 2023-06-13 04:37:42; prompt: \"Answer the question ...\"; response: \"A langchain `Agent` ...\";  cost: $0.000706; total_tokens: 353; metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "timestamp: 2023-06-13 04:37:44; prompt: \"Summarize the follow...\"; response: \"Langchain Agent sele...\";  cost: $0.000186; total_tokens: 93; metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chain.history:\n",
    "    print(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example showing history and usages/costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Shane! How can I assist you today?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat = OpenAIChat(model_name='gpt-3.5-turbo', temperature=0)\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model object tracks usage/cost data across all messages  \n",
    "def print_usage(model: OpenAIChat):\n",
    "    usage = f\"\"\"\n",
    "    Total Cost: ${model.total_cost:.6f}\n",
    "    Total Tokens: {model.total_tokens:,}\n",
    "    Total Prompt Tokens: {model.total_prompt_tokens:,}\n",
    "    Total Response Tokens: {model.total_response_tokens:,}\n",
    "    \"\"\"\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000072\n",
      "    Total Tokens: 36\n",
      "    Total Prompt Tokens: 26\n",
      "    Total Response Tokens: 10\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageMetaData: timestamp: 2023-06-13 02:13:15; prompt: \"Hi, my name is Shane...\"; response: \"Hello Shane! How can...\";  total_tokens: 36; cost: $0.000072 metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "previous prompt: Hi, my name is Shane.\n",
      "previous response: Hello Shane! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:13:15; prompt: \"Hi, my name is Shane...\"; response: \"Hello Shane! How can...\";  total_tokens: 36; cost: $0.000072 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# the `history` property contains a list of `MessageMetaData` objects for each message (i.e.\n",
    "# prompt & response) which contains usage/cost data for that message.\n",
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Shane.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Shane.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-06-13 02:13:15; prompt: \"Hi, my name is Shane...\"; response: \"Hello Shane! How can...\";  total_tokens: 36; cost: $0.000072 metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "timestamp: 2023-06-13 02:16:42; prompt: \"Do you remember my n...\"; response: \"Yes, your name is Sh...\";  total_tokens: 59; cost: $0.000118 metadata: {'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "for record in chat.history:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Shane.'},\n",
       " {'role': 'assistant', 'content': 'Hello Shane! How can I assist you today?'},\n",
       " {'role': 'user', 'content': 'Do you remember my name?'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageMetaData: timestamp: 2023-06-13 02:16:42; prompt: \"Do you remember my n...\"; response: \"Yes, your name is Sh...\";  total_tokens: 59; cost: $0.000118 metadata: {'model_name': 'gpt-3.5-turbo'}\n",
      "previous prompt: Do you remember my name?\n",
      "previous response: Yes, your name is Shane.\n"
     ]
    }
   ],
   "source": [
    "# You can get the last MessageMetaData via: \n",
    "print(f\"MessageMetaData: {chat.previous_message}\")\n",
    "# Or you can get the last prompt/response\n",
    "print(f\"previous prompt: {chat.previous_prompt}\")\n",
    "print(f\"previous response: {chat.previous_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000190\n",
      "    Total Tokens: 95\n",
      "    Total Prompt Tokens: 78\n",
      "    Total Response Tokens: 17\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `OpenAIChat` model has a `memory_strategy` parameter and takes a `MemoryBuffer` class. A `MemoryBuffer` class is a callable that takes a `list[MessageMetaData]` (i.e. from the `model.history` property) and also returns a `list[MessageMetaData]` serving as the model's memory (i.e. a list containing the messages that will be sent to the model along with the new prompt). This allows the end user to easily define a memory strategy of their own (e.g. keep the first message and the last `n` messages).\n",
    "\n",
    "One Example of a `MemoryBuffer` is a `MemoryBufferMessageWindow` class where you can specify the last `n` messages that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Shane! How can I assist you today?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "from llm_chain.memory import MemoryBufferMessageWindow\n",
    "\n",
    "chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0,\n",
    "    memory_strategy=MemoryBufferMessageWindow(last_n_messages=0),  # no memory\n",
    ")\n",
    "response = chat(\"Hi, my name is Shane.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000072\n",
      "    Total Tokens: 36\n",
      "    Total Prompt Tokens: 26\n",
      "    Total Response Tokens: 10\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Shane.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI language model, I don't have the ability to remember specific information about individual users. However, I'm always here to assist you with any questions or tasks you may have.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(\"Do you remember my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MessageMetaData(prompt='Hi, my name is Shane.', response='Hello Shane! How can I assist you today?', metadata={'model_name': 'gpt-3.5-turbo'}, prompt_tokens=26, response_tokens=10, total_tokens=36, cost=7.2e-05),\n",
       " MessageMetaData(prompt='Do you remember my name?', response=\"I'm sorry, but as an AI language model, I don't have the ability to remember specific information about individual users. However, I'm always here to assist you with any questions or tasks you may have.\", metadata={'model_name': 'gpt-3.5-turbo'}, prompt_tokens=25, response_tokens=43, total_tokens=68, cost=0.000136)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we still have access to the full history, but the ChatGPT didn't use any of it.\n",
    "chat._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Do you remember my name?'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also see the exact messages sent to ChatGPT\n",
    "chat._previous_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total Cost: $0.000208\n",
      "    Total Tokens: 104\n",
      "    Total Prompt Tokens: 51\n",
      "    Total Response Tokens: 53\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# NOTE: since we created a new OpenAIChat object, the costs/usage are reset\n",
    "print_usage(model=chat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
