{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools and more\n",
    "\n",
    "Before showning an example using a `Chain`, it will be helpful to understand the various tools that can be used in a chain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo Search\n",
    "\n",
    "We can use the DuckDuckGo search engine to retrieve the top URLs associated with a search query. The `DuckDuckGoSearch` object is a callable object that returns a list of dictionaries. Each item in the list corresponds to a search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What are LLMs, and how are they used in generative AI?',\n",
       "  'href': 'https://www.computerworld.com/article/3697649/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html',\n",
       "  'body': \"Large language models are the algorithmic basis for chatbots like OpenAI's ChatGPT and Google's Bard. The technology is tied back to billions — even trillions — of parameters that can make them...\"},\n",
       " {'title': 'What is a large language model and how does it work? - Fast Company',\n",
       "  'href': 'https://www.fastcompany.com/90884581/what-is-a-large-language-model',\n",
       "  'body': 'Large language models are the foundational technology behind recent artificial intelligence advancements like ChatGPT.'},\n",
       " {'title': 'What are Large Language Models - MachineLearningMastery.com',\n",
       "  'href': 'https://machinelearningmastery.com/what-are-large-language-models/',\n",
       "  'body': 'Large language models (LLMs) are recent advances in deep learning models to work on human languages. Some great use case of LLMs has been demonstrated. A large language model is a trained deep-learning model that understands and generates text in a human-like fashion. Behind the scene, it is a large transformer model that does all the magic.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.tools import DuckDuckGoSearch\n",
    "\n",
    "duckduckgo_search = DuckDuckGoSearch(top_n=3)\n",
    "duckduckgo_search(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A `Chain` consists of individual links. Each link can be thought of as a task in a workflow (e.g. document search, web search, or chat model). Each link is a callable (either a function or a callable object) where the output of one link is the input of the next link. So a `Chain` is a simple mechanism that takes an input and sends the input to the first link, and the propegates the output of the first link to the second link, until the end of the chain is reached, and returns the final result.** \n",
    "\n",
    "A `Chain` object aggregates the history (messages, usage, etc) of all of the links. More specifically, it aggregiates all of the lists of `Record` objects for any link that has a `history` property, such as the OpenAIChat class. The `Chain` class provides various history properties (e.g. `history`, `usage_history`, `message_history`) depending on the type of `Record` returned by each link's history property (e.g. the `usage_history` property returns all of the `UsageRecord` objects across any link whose `history` property returns `UsageRecord`s)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat model w/ web search "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will ask a chat model a question, and provide the model with additional context based on the most relevant text we find in a web-search. For the sake of the example, we will ask the chat model to the answer it provides and summarize that answer.  The workflow is defined as follows:\n",
    "\n",
    "\n",
    "|   input   |  output  |   link/task   |\n",
    "|-----------|----------|----------|\n",
    "|  `None` |   str    |  Ask a question: `What is the meaning of life?`  |\n",
    "|    str    |   urls   |  Do a web-search via `DuckDuckGo` based on the question.  |\n",
    "|   urls    | Documents|  Take the urls, scrape the corresponding web-pages, and convert to a list of `Document` objects  |\n",
    "| Documents | Documents|  Split the `Document` objects into smaller chunks (also `Document` objects)  |\n",
    "| Documents | `None` |  Store the Documents in a `Chroma` document index so that we can lookup the most relevant documents |\n",
    "| `None`  |   str    |  Return the original question `What is the meaning of life?` |\n",
    "|    str    |   str    |  Take the original question, lookup the most relevant documents, and construct the corresponding prompt with the documents injected into the prompt. |\n",
    "|    str    |   str    |  Pass the prompt to the chat model; receive a response |\n",
    "|    str    |   str    |  Construct a new prompt, containing the response and a request to summarize the response. |\n",
    "|    str    |   str    |  Pass the new prompt to the chat model; receive a response |\n",
    "\n",
    "**Notice how the output of one link matches the input of the next link.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the chain and corresponding objects in the next cell:\n",
    "\n",
    "- the `ChromaDocumentIndex` is an object that has two main methods: `add()` to add Document objects to the search index and `search()` to search for Documents based on a provided Document. It's a callable object that when called with a list of Documents, adds the Documents to the index, and when called with a single Document, searches the index and returns a list of similar Documents.\n",
    "- the `DocSearchTemplate` is a callable object that is initialized with a document index and, when called (with a `str` input representing the original question/prompt), looks up the most similar/relevant documents based on the query, and returns a new prompt with the original question and with the most relevant chunks.\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial objects in chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.base import Document, Chain, Value\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import DuckDuckGoSearch, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# Seach DuckDuckGo based on question\n",
    "duckduckgo_search = DuckDuckGoSearch(top_n=3)\n",
    "# Embeddings model used for document index/search\n",
    "embeddings_model = OpenAIEmbeddings(model_name='text-embedding-ada-002')\n",
    "# Store and lookup most relevant documents\n",
    "document_index = ChromaDocumentIndex(embeddings_model=embeddings_model, n_results=3)\n",
    "# DocSearchTemplate looks up most relevant documents based on query\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=3)\n",
    "# Create a chat model without streaming.\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# Create a chat model with streaming enabled via callback.\n",
    "streaming_chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    streaming_callback=lambda x: print(x.response, end='|'),\n",
    ")\n",
    "\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    \"\"\"For each url (`href` in `results`), extracts text, cleans, returns Document.\"\"\"\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "# A `Value` object is a simple caching mechanism. It is a callable object that, \n",
    "# when called with a value passed in, caches the value in an instance field and returns the value;\n",
    "# and when called without a value passed in returns the cached value stored in the instance field.\n",
    "# In this case, it's a way for us to pass our question to the `DuckDuckGoSearch` object and then\n",
    "# also pass it to the `DocSearchTemplate` object in the middle of the chain.\n",
    "initial_question = Value()\n",
    "# This simple function takes the response from the original chat model and creates a prompt that\n",
    "# asks the model to summarize the response.\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChromaDocumentIndex.search() got an unexpected keyword argument 'n_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m      2\u001b[0m chain \u001b[39m=\u001b[39m Chain(links\u001b[39m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m     initial_question,\n\u001b[1;32m      4\u001b[0m     duckduckgo_search,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     streaming_chat,\n\u001b[1;32m     13\u001b[0m ])\n\u001b[1;32m     14\u001b[0m \u001b[39m# the value passed into `chain()` is passed to the `initial_question` object (which\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# is `Value` object and so it caches the value passed in and also returns it) which then gets\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# passed to the `duckduckgo_search` object, and so on.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# the response of the final model is streamed, but should also match the response returned\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response \u001b[39m=\u001b[39m chain(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is the meaning of life?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/code/llm_chain/base.py:338\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_links) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    337\u001b[0m     \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_links[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m--> 338\u001b[0m         result \u001b[39m=\u001b[39m link(result)\n\u001b[1;32m    339\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/code/llm_chain/prompt_templates.py:41\u001b[0m, in \u001b[0;36mDocSearchTemplate.__call__\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(prompt)\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilar_docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_doc_index\u001b[39m.\u001b[39;49msearch(\n\u001b[1;32m     42\u001b[0m         doc\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     43\u001b[0m         n_results\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_docs,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     doc_string \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([x\u001b[39m.\u001b[39mcontent \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilar_docs])\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate\u001b[39m.\u001b[39m\\\n\u001b[1;32m     47\u001b[0m         replace(\u001b[39m'\u001b[39m\u001b[39m{{\u001b[39m\u001b[39mdocuments}}\u001b[39m\u001b[39m'\u001b[39m, doc_string)\u001b[39m.\u001b[39m\\\n\u001b[1;32m     48\u001b[0m         replace(\u001b[39m'\u001b[39m\u001b[39m{{\u001b[39m\u001b[39mprompt}}\u001b[39m\u001b[39m'\u001b[39m, prompt)\n",
      "\u001b[0;31mTypeError\u001b[0m: ChromaDocumentIndex.search() got an unexpected keyword argument 'n_results'"
     ]
    }
   ],
   "source": [
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    initial_question,\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # split web-pages into smaller chunks; defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    initial_question,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "# the value passed into `chain()` is passed to the `initial_question` object (which\n",
    "# is `Value` object and so it caches the value passed in and also returns it) which then gets\n",
    "# passed to the `duckduckgo_search` object, and so on.\n",
    "# the response of the final model is streamed, but should also match the response returned\n",
    "response = chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_chain.base import Document, Chain, Value\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import DuckDuckGoSearch, html_page_loader, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "duckduckgo_search = DuckDuckGoSearch(top_n=3)\n",
    "\n",
    "document_index = ChromaDocumentIndex(embeddings_model=OpenAIEmbeddings(model_name='text-embedding-ada-002'))\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=2)\n",
    "# OpenAI Chat model\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "streaming_callback = lambda x: print(x.response, end='|')\n",
    "streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo', streaming_callback=streaming_callback)\n",
    "\n",
    "# for each url, extracts text, cleans, returns doc\n",
    "def search_results_to_docs(results: list[dict]) -> list[Document]:\n",
    "    return [Document(content=html_page_loader(x['href']).replace('\\n', ' ')) for x in results]\n",
    "\n",
    "initial_question = Value()\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'\n",
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    initial_question,\n",
    "    duckduckgo_search,\n",
    "    search_results_to_docs,\n",
    "    split_documents,  # defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() or search() based on input\n",
    "    initial_question,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "\n",
    "response = chain(\"What is the meaning of life?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
