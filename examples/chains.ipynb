{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from notebook_helpers import usage_string, mprint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `chain` is an object that executes a sequence of tasks called `links`. Each link in the chain is a callable, which can be either a function or an object that implements the `__call__` method. **The output of one link serves as the input to the next link in the chain. So a `Chain` is a simple mechanism that takes an input and sends the input to the first link, and the propegates the output of the first link to the second link, and so on, until the end of the chain is reached, and returns the final result.** \n",
    "\n",
    "Furthermore, a chain aggregates the history (prompts/responses, token usage, costs, etc.) across all links. More specifically, it aggregates all of the `Record` objects across any link that has a `history` property (which returns a list of Record objects; a Record object contains the metadata of an event such as costs, tokens used, prompt, response, etc.). This functionality allows the chain to contain convenient properties that aggregate the costs and usage across all links of the chain, and can also be used to explore intermediate steps and events in the chain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Using a model as a prompt enhancer\n",
    "\n",
    "Here's an example of a simple \"prompt enhancer\", where the first model enhances the user's prompt and the second model provides a response based on the enhanced prompt.\n",
    "\n",
    "- first link: defines a prompt-template that takes the user's prompt, and creates a new prompt asking a chat model to improve the prompt (within the context of creating python code)\n",
    "- second link: the model that takes the modified prompt and improves the prompt\n",
    "- third link: the model used for the chat/assistant; takes the response from the last model (which is an improved prompt) and returns the request\n",
    "- fourth link: ignores the response from the chat model, creates a new prompt asking the chat model to extract the code created in the previous response\n",
    "- fifth link: chat model, which internally maintains the the history of messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def replace_whitespace(input_string: str) -> str:\n",
      "    \"\"\"\n",
      "    Replaces any whitespace character in the input string with a single space.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input string to process.\n",
      "\n",
      "    Returns:\n",
      "        str: The processed string with whitespace characters replaced by a single space.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input parameter is not a string.\n",
      "\n",
      "    Examples:\n",
      "        >>> replace_whitespace(\"Hello\\tworld\\n\")\n",
      "        'Hello world '\n",
      "        >>> replace_whitespace(\"   multiple   spaces   \")\n",
      "        ' multiple spaces '\n",
      "\n",
      "    Additional Considerations:\n",
      "        - Leading and trailing whitespace characters will be preserved.\n",
      "        - Multiple consecutive whitespace characters will be replaced by a single space.\n",
      "    \"\"\"\n",
      "    if not isinstance(input_string, str):\n",
      "        raise TypeError(\"Input parameter must be a string.\")\n",
      "\n",
      "    return ' '.join(input_string.split())\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from llm_chain.base import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "prompt_enhancer = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# different model/object, therefore different message history (i.e. conversation)\n",
    "chat_assistant = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "\n",
    "def prompt_template(user_prompt: str) -> str:\n",
    "    return \"Improve the user's request, below, by expanding the request \" \\\n",
    "        \"to describe the relevant python best practices and documentation \" \\\n",
    "        f\"requirements that should followed:\\n\\n```{user_prompt}```\"\n",
    "\n",
    "def prompt_extract_code(_) -> str:\n",
    "    # `_` signals that we are ignoring the input (from the previous link)\n",
    "    return \"Return only the primary code of interest from the previous answer, \"\\\n",
    "        \"including docstrings, but without any text/response.\"\n",
    "\n",
    "# the only requirement for the list is that each item/link is a callable\n",
    "# where the output of one link matches the input of the next link\n",
    "# input to the chain is passed to the first link\n",
    "# the output of the last link is returned by the chain\n",
    "chain = Chain(links=[\n",
    "    prompt_template,      # modifies the user's prompt\n",
    "    prompt_enhancer,      # returns an improved version of the user's prompt\n",
    "    chat_assistant,       # returns the chat response based on the improved prompt\n",
    "    prompt_extract_code,  # prompt to ask the model to extract only the relevant code\n",
    "    chat_assistant,       # returns only the function from the model's last response\n",
    "])\n",
    "prompt = \"create a function to replace any whitespace character with a single space\"\n",
    "response = chain(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:            $0.0035\n",
      "Total Tokens:     2,016\n",
      "Prompt Tokens:    1,142\n",
      "Response Tokens:  874\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost:            ${chain.cost:.4f}\")\n",
    "print(f\"Total Tokens:     {chain.total_tokens:,}\")\n",
    "print(f\"Prompt Tokens:    {chain.prompt_tokens:,}\")\n",
    "print(f\"Response Tokens:  {chain.response_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the history of the chain (i.e. the aggregated history across all links) with the `chain.history` property. \n",
    "\n",
    "In this example, the only class that tracks history is `OpenAIChat`. Therefore, both the `prompt_enhancer` and `chat_assistant` objects will contain history. `chain.history` will return a list of three `ExchangeRecord` objects. The first record corresponds to our request to the `prompt_enhancer`, and the second two records correspond to our `chat_assistant` requests. An ExchangeRecord represents a single exchange/transaction with an LLM, encompassing an input (prompt) and its corresponding output (response), along with other properties like cost and token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[llm_chain.base.ExchangeRecord,\n",
       " llm_chain.base.ExchangeRecord,\n",
       " llm_chain.base.ExchangeRecord]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in chain.history]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the response we received from the `prompt_enhancer` model by looking at the first record's `response` property (or the second record's `prompt` property since we pass the output of `prompt_enhancer` as the input to the `chat_assistant`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please create a Python function that adheres to the following best practices and documentation requirements:\n",
       "\n",
       "1. Function Name: Choose a descriptive and meaningful name for the function, following the Python naming conventions (lowercase with words separated by underscores).\n",
       "\n",
       "2. Function Parameters: Define the function to accept a single parameter, which should be a string.\n",
       "\n",
       "3. Return Type: Specify the return type of the function as a string.\n",
       "\n",
       "4. Function Description: Provide a clear and concise description of what the function does, including its purpose and expected behavior.\n",
       "\n",
       "5. Input Validation: Consider adding input validation to ensure that the input parameter is a string. You can use the `isinstance()` function to check if the input is of type `str`.\n",
       "\n",
       "6. Algorithm: Describe the algorithm or approach used to replace any whitespace character with a single space. Consider using Python's built-in string methods or regular expressions for efficient and concise code.\n",
       "\n",
       "7. Error Handling: Handle any potential errors or exceptions that may occur during the execution of the function. For example, if the input parameter is not a string, you can raise a `TypeError` with an appropriate error message.\n",
       "\n",
       "8. Examples: Provide one or more examples of how to use the function, including both valid and invalid inputs, along with the expected output.\n",
       "\n",
       "9. Additional Considerations: Consider any additional considerations or edge cases that may need to be addressed, such as handling leading/trailing whitespace or multiple consecutive whitespace characters.\n",
       "\n",
       "By following these best practices and documenting your code effectively, you will create a function that is easier to understand, maintain, and collaborate on with other developers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(chain.history[0].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also view the original response from the `chat_assistant` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is an example of a Python function that adheres to the best practices and documentation requirements mentioned:\n",
       "\n",
       "```python\n",
       "def replace_whitespace(input_string: str) -> str:\n",
       "    \"\"\"\n",
       "    Replaces any whitespace character in the input string with a single space.\n",
       "\n",
       "    Args:\n",
       "        input_string (str): The input string to process.\n",
       "\n",
       "    Returns:\n",
       "        str: The processed string with whitespace characters replaced by a single space.\n",
       "\n",
       "    Raises:\n",
       "        TypeError: If the input parameter is not a string.\n",
       "\n",
       "    Examples:\n",
       "        >>> replace_whitespace(\"Hello\\tworld\\n\")\n",
       "        'Hello world '\n",
       "        >>> replace_whitespace(\"   multiple   spaces   \")\n",
       "        ' multiple spaces '\n",
       "\n",
       "    Additional Considerations:\n",
       "        - Leading and trailing whitespace characters will be preserved.\n",
       "        - Multiple consecutive whitespace characters will be replaced by a single space.\n",
       "    \"\"\"\n",
       "    if not isinstance(input_string, str):\n",
       "        raise TypeError(\"Input parameter must be a string.\")\n",
       "\n",
       "    return ' '.join(input_string.split())\n",
       "\n",
       "```\n",
       "\n",
       "In this example, the function `replace_whitespace` takes a single parameter `input_string` of type `str` and returns a processed string with whitespace characters replaced by a single space.\n",
       "\n",
       "The function uses the `isinstance()` function to validate that the input parameter is a string. If it is not, a `TypeError` is raised with an appropriate error message.\n",
       "\n",
       "The algorithm used to replace whitespace characters is as follows:\n",
       "1. The `split()` method is used to split the input string into a list of substrings, using whitespace characters as separators.\n",
       "2. The `join()` method is used to join the substrings in the list with a single space as the separator.\n",
       "\n",
       "The function includes examples of how to use it, including valid and invalid inputs, along with the expected output. It also mentions additional considerations, such as preserving leading/trailing whitespace and replacing multiple consecutive whitespace characters with a single space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(chain.history[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The final response returned by the `chat_assistant` (and by the `chain` object) returns only the `replace_whitespace` function. The `response` object should match the `response` value in the last record (`chain.history[-1].response`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert response == chain.history[-1].response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example -- Using the results of a web-search in a chat message."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will ask a chat model a question, and provide the model with additional context based on the most relevant text we find in a web-search. For the sake of the example, we will ask the chat model to the answer it provides and summarize that answer.  The workflow is defined as follows:\n",
    "\n",
    "\n",
    "|   input   |  output  |   link/task   |\n",
    "|-----------|----------|----------|\n",
    "|  `None` |   str    |  Ask a question: e.g. `\"What is the meaning of life?\"`  |\n",
    "|    str    |   urls   |  Do a web-search via `DuckDuckGo` based on the question.  |\n",
    "|   urls    | Documents|  Take the urls, scrape the corresponding web-pages, and convert to a list of `Document` objects  |\n",
    "| Documents | Documents|  Split the `Document` objects into smaller chunks (also `Document` objects)  |\n",
    "| Documents | `None` |  Store the Documents in a `Chroma` document index so that we can lookup the most relevant documents |\n",
    "| `None`  |   str    |  Return the original question `What is the meaning of life?` |\n",
    "|    str    |   str    |  Take the original question, lookup the most relevant documents, and construct the corresponding prompt with the documents injected into the prompt. |\n",
    "|    str    |   str    |  Pass the prompt to the chat model; receive a response |\n",
    "|    str    |   str    |  Construct a new prompt, containing the response and a request to summarize the response. |\n",
    "|    str    |   str    |  Pass the new prompt to the chat model; receive a response |\n",
    "\n",
    "**Notice how the output of one link matches the input of the next link. This means you could swap out any of the objects we use below with your own custom function or class and the only requirement is that the input/output matches for that particular link.**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial objects in chain\n",
    "\n",
    "Let's define the objects and functions we are going to include in the chain. We're including many links/tasks so that we can see how flexible our chain can be.\n",
    "\n",
    "See the [tools.ipynb](https://github.com/shane-kercheval/llm-chain/tree/main/examples/tools.ipynb) notebook for examples of how the various helper classes below (e.g. `DuckDuckGoSearch`, `scrape_url`, etc.) are used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick note on the `Value` object.\n",
    "\n",
    "The only bit of magic below is that we're using a `Value` object to cache the initial question, feed it into the web-search, and then inject it back into the chain at a later point (passing it to the prompt-template). A `Value` object is a callable that, when called with a value caches and returns the same value, and when called without a value, simply returns the previously cached value. When you understand what it's doing, it's not magic at all. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a value\n",
      "This is a value\n",
      "new value\n",
      "new value\n"
     ]
    }
   ],
   "source": [
    "from llm_chain.base import Value\n",
    "cache = Value()\n",
    "result = cache(\"This is a value\")  # calling the object with a value caches and returns the value\n",
    "print(result)  # the value in `result` is whatever was passed in to the Value object\n",
    "print(cache())  # calling the object without a parameter returns the cached value\n",
    "result = cache(\"new value\")\n",
    "print(result)\n",
    "print(cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llm_chain.base import Document, Chain, Value\n",
    "from llm_chain.models import OpenAIEmbedding, OpenAIChat\n",
    "from llm_chain.tools import DuckDuckGoSearch, scrape_url, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# Seach DuckDuckGo based on initial question (passed into the chain)\n",
    "duckduckgo_search = DuckDuckGoSearch(top_n=3)\n",
    "\n",
    "# define a function that takes the links from the web-search, scrapes the web-pages,\n",
    "# and then creates Document objects from the text of each web-page\n",
    "def scrape_urls(search_results):\n",
    "    \"\"\"\n",
    "    For each url (i.e. `href` in `search_results`):\n",
    "    - extracts text\n",
    "    - replace new-lines with spaces\n",
    "    - create a Document object\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Document(content=re.sub(r'\\s+', ' ', scrape_url(x['href'])))\n",
    "        for x in search_results\n",
    "    ]\n",
    "\n",
    "# Embeddings model used for document index/search (the documents created from the web-search)\n",
    "embeddings_model = OpenAIEmbedding(model_name='text-embedding-ada-002')\n",
    "document_index = ChromaDocumentIndex(embeddings_model=embeddings_model, n_results=3)\n",
    "# DocSearchTemplate uses the ChromaDocumentIndex object to search for the most relevant documents\n",
    "# (from the web-search) based on the intitial question (which it takes as an input)\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=3)\n",
    "# Create a chat model without streaming.\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# Create a chat model with streaming enabled via callback.\n",
    "streaming_chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    streaming_callback=lambda x: print(x.response, end='|'),\n",
    ")\n",
    "\n",
    "# A `Value` object is a simple caching mechanism. It's a callable that, when passed a value, it\n",
    "# caches and returns that value; and when called without a value, it returns the cached value.\n",
    "# Below, it's being used to cache the original question, feed the question into the web-search\n",
    "# (`DuckDuckGoSearch`), and then re-inject the question back in the chain and into the\n",
    "# prompt-template (`DocSearchTemplate`).\n",
    "question = Value()\n",
    "# This simple function takes the response from the original chat model and creates a prompt that\n",
    "# asks the model to summarize the response.\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Running the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang|Chain| is| a| platform| with| tools| and| APIs| for| creating| applications| using| Language| Models| (|LL|Ms|).|"
     ]
    }
   ],
   "source": [
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    question,\n",
    "    duckduckgo_search,\n",
    "    scrape_urls,\n",
    "    split_documents,  # split web-pages into smaller chunks; defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() if given a list of documents (which is returned by `split_documents`)\n",
    "    question,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "# the value passed into `chain()` is passed to the `initial_question` object (which\n",
    "# is `Value` object and so it caches the value passed in and also returns it) which then gets\n",
    "# passed to the `duckduckgo_search` object, and so on.\n",
    "# the response of the final model is streamed, because our chat model has the streaming_callback\n",
    "# set, but it should also match the response returned\n",
    "response = chain(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> LangChain is a platform with tools and APIs for creating applications using Language Models (LLMs)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total costs and usage\n",
    "\n",
    "The `Chain` object aggregates the costs/usage across all links that have a `history` property where that `history` property returns `UsageRecord` objects. In other words, any link that tracks its own history automatically gets counted towards the total usage within the Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:              $0.00182\n",
      "Total Tokens:       11,975\n",
      "Prompt Tokens:      391\n",
      "Response Tokens:    40\n",
      "Embedding Tokens:   11,544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usage_string simply uses the properties in chain (chain.cost, chain.total_tokens, etc.)\n",
    "# to create a formatted string\n",
    "print(usage_string(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11544"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.embedding_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "Similar to tracking the costs/usage, we can dig into the history at a more granular level.\n",
    "\n",
    "As you can see below, the `history` property returns a list containing:\n",
    "\n",
    "- a `SearchRecord` object capturing our original search query and results\n",
    "- two `EmbeddingsRecord` records; the first corresponds to getting the embeddings of all of the chunks from the web-pages; the second corresponds to getting the embedding of our original question so that we can find the most relevant chunks; the `EmbeddingsRecord` is a `UsageRecord` so it contains costs/usage\n",
    "- two `MessageRecord` records; the first corresponds to the first question (prompt & response) we made to the chat model; the second corresponds to our second query to the chat model asking it to summarize the first response; the `MessageRecord` is a `UsageRecord` so it contains costs/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[llm_chain.tools.SearchRecord,\n",
       " llm_chain.base.EmbeddingRecord,\n",
       " llm_chain.base.EmbeddingRecord,\n",
       " llm_chain.base.ExchangeRecord,\n",
       " llm_chain.base.ExchangeRecord]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in chain.history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-07 00:43:32.185; prompt: \"Answer the question at the end of the text as trut...\"; response: \"LangChain is a platform that provides tools and AP...\";  cost: $0.000552; total_tokens: 361; prompt_tokens: 340; response_tokens: 21; uuid: dd21d99d-5e92-45c5-a187-43d7502f0336\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-07 00:43:33.126; prompt: \"Summarize the following in less than 20 words: \"La...\"; response: \"LangChain is a platform with tools and APIs for cr...\";  cost: $0.000114; total_tokens: 70; prompt_tokens: 51; response_tokens: 19; uuid: 27e691f8-ffcc-4894-8750-0d381e6e9547\n"
     ]
    }
   ],
   "source": [
    "print(chain.history[3])\n",
    "mprint('---')\n",
    "print(chain.history[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `exchange_history`\n",
    "\n",
    "We can use the `exchange_history` property which simply returns all of the history items that are of type `ExchangeRecord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain.exchange_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-07 00:43:32.185; prompt: \"Answer the question at the end of the text as trut...\"; response: \"LangChain is a platform that provides tools and AP...\";  cost: $0.000552; total_tokens: 361; prompt_tokens: 340; response_tokens: 21; uuid: dd21d99d-5e92-45c5-a187-43d7502f0336\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-07 00:43:33.126; prompt: \"Summarize the following in less than 20 words: \"La...\"; response: \"LangChain is a platform with tools and APIs for cr...\";  cost: $0.000114; total_tokens: 70; prompt_tokens: 51; response_tokens: 19; uuid: 27e691f8-ffcc-4894-8750-0d381e6e9547\n"
     ]
    }
   ],
   "source": [
    "print(chain.exchange_history[0])  # same as chain.history[3]\n",
    "mprint('---')\n",
    "print(chain.exchange_history[1])  # same as chain.history[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First prompt to the chat model\n",
    "\n",
    "You can see below that we used three chunks from our original web-search in the first prompt we sent to ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "credentials. By incorporating LLMs through LangChain, developers can create applications that provide more natural and context-aware interactions with users, resulting in enhanced user experiences and improved engagement. Frequently Asked Questions Q1. What is LangChain? A1. LangChain is a platform that provides tools and APIs for building applications powered by Language Models (LLMs). It simplifies the integration of LLMs into your projects, enabling you to leverage advanced language\n",
       "\n",
       "With LangChain you can seamlessly integrate LLMs into your projects, harnessing their extraordinary capabilities. Let’s embark on an exhilarating journey, exploring the captivating features and boundless possibilities that LangChain unveils. LangChain is an advanced platform that provides developers with a seamless and intuitive interface to leverage the power of LLM in their applications. It offers a range of APIs and tools that simplify the integration of LLM into your projects, enabling you\n",
       "\n",
       "power for various text generation and understanding tasks. What can be done with Langchain? LangChain, with its diverse set of features, offers developers a wide range of possibilities to explore and leverage in their applications. Let’s dive into the key components of LangChain—models, prompts, chains, indexes, and memory and discover what can be accomplished with each. Models Numerous new LLMs are currently emerging. LangChain provides a streamlined interface and integrations for various\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "What is langchain?\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(chain.exchange_history[0].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First response from the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> LangChain is a platform that provides tools and APIs for building applications powered by Language Models (LLMs)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + chain.exchange_history[0].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second prompt to the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summarize the following in less than 20 words: \"LangChain is a platform that provides tools and APIs for building applications powered by Language Models (LLMs).\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(chain.exchange_history[1].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second response from the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> LangChain is a platform with tools and APIs for creating applications using Language Models (LLMs)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + chain.exchange_history[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
