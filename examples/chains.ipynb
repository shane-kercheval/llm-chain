{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from notebook_helpers import usage_string, mprint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `chain` is an object that executes a sequence of tasks called `links`. Each link in the chain is a callable, which can be either a function or an object that implements the `__call__` method. **The output of one link serves as the input to the next link in the chain. So a `Chain` is a simple mechanism that takes an input and sends the input to the first link, and the propegates the output of the first link to the second link, and so on, until the end of the chain is reached, and returns the final result.** \n",
    "\n",
    "Furthermore, a chain aggregates the history (prompts/responses, token usage, costs, etc) across all links. More specifically, it aggregiates all of the `Record` objects across any link that has a `history` property (which returns a list of Record objects; a Record object contains the metadata of an event like cost, tokens used, prompt, response, etc.). This functionality allows the chain to contain convenient properties that aggregate the costs and usage across all links of the chain, and can also be used to explore intermediate steps and events in the chain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "\n",
    "Here's a simple example demonstrating llm-chain. A more in-depth example with additional explanation can be found below.\n",
    "\n",
    "- Ask the chat model a question (\"What is the meaning of life?\")\n",
    "- The model responds, and the response is sent to the next link, which creates and returns a new prompt indicating that the link's input (which is the output from the last link; i.e. the model's response) should be summarized.\n",
    "- The new prompt is sent to the next link, which is the chat model, and the response is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is a philosophical question that has been debated for centuries, with different beliefs and interpretations across cultures. Some find meaning in happiness and fulfillment, while others seek it through religious or spiritual beliefs, ultimately making it a subjective and individual pursuit.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.base import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat_model = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# each link is a callable where the output of one link is the input to the next link\n",
    "chain = Chain(links=[\n",
    "    chat_model,\n",
    "    lambda x: f\"Summarize the following in two sentences: ```{x}```\",\n",
    "    chat_model,\n",
    "])\n",
    "chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:           $0.00063\n",
      "Total Tokens:    375\n",
      "Prompt Tokens:   239\n",
      "Response Tokens: 136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usage_string simply uses the properties in chain (chain.cost, chain.total_tokens, etc.)\n",
    "# to create a formatted string\n",
    "print(usage_string(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain.exchange_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> What is the meaning of life?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is a philosophical question that has been debated for centuries. Different people and cultures have different beliefs and interpretations. Some believe that the meaning of life is to seek happiness and fulfillment, while others find meaning in religious or spiritual beliefs. Ultimately, the meaning of life may be subjective and can vary from person to person. It is up to each individual to explore and find their own sense of purpose and meaning in life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Summarize the following in two sentences: ```The meaning of life is a philosophical question that has been debated for centuries. Different people and cultures have different beliefs and interpretations. Some believe that the meaning of life is to seek happiness and fulfillment, while others find meaning in religious or spiritual beliefs. Ultimately, the meaning of life may be subjective and can vary from person to person. It is up to each individual to explore and find their own sense of purpose and meaning in life.```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is a philosophical question that has been debated for centuries, with different beliefs and interpretations across cultures. Some find meaning in happiness and fulfillment, while others seek it through religious or spiritual beliefs, ultimately making it a subjective and individual pursuit."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + chain.exchange_history[0].prompt)\n",
    "mprint(\"> \" + chain.exchange_history[0].response)\n",
    "mprint(\"> \" + chain.exchange_history[1].prompt)\n",
    "mprint(\"> \" + chain.exchange_history[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the exchange history above (which is sent to the OpenAI model), that in the second link (the line with the lambda function) we don't actually have to use the response (`x` in lambda) since it's already in the history. I simply did that for illustrative purposes. We could replace the second link with `lambda _: \"Summarize your previous answer in two sentences.\"`, which ignores the ouput of the first link (i.e. first response from chat model) and would actually reduce the number of tokens we use since we aren't passing the previous response in the new exchange. Let's do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is a subjective and philosophical question that has been debated for centuries. It varies from person to person and can be found in seeking happiness, fulfillment, or through religious or spiritual beliefs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.base import Chain\n",
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "chat_model = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "chain = Chain(links=[\n",
    "    chat_model,\n",
    "    lambda _: \"Summarize your previous answer in two sentences.\",\n",
    "    chat_model,\n",
    "])\n",
    "chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:           $0.00048\n",
      "Total Tokens:    278\n",
      "Prompt Tokens:   152\n",
      "Response Tokens: 126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(usage_string(chain))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the cost/token-usage is lower since we're not sending information that is already contained in the previous response. Compare the history below to the history above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> What is the meaning of life?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is a philosophical question that has been debated for centuries. Different people and cultures have different beliefs and interpretations. Some believe that the meaning of life is to seek happiness and fulfillment, while others find meaning in religious or spiritual beliefs. Ultimately, the meaning of life may be subjective and can vary from person to person. It is up to each individual to explore and find their own sense of purpose and meaning in life."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Summarize your previous answer in two sentences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is a subjective and philosophical question that has been debated for centuries. It varies from person to person and can be found in seeking happiness, fulfillment, or through religious or spiritual beliefs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + chain.exchange_history[0].prompt)\n",
    "mprint(\"> \" + chain.exchange_history[0].response)\n",
    "mprint(\"> \" + chain.exchange_history[1].prompt)\n",
    "mprint(\"> \" + chain.exchange_history[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will ask a chat model a question, and provide the model with additional context based on the most relevant text we find in a web-search. For the sake of the example, we will ask the chat model to the answer it provides and summarize that answer.  The workflow is defined as follows:\n",
    "\n",
    "\n",
    "|   input   |  output  |   link/task   |\n",
    "|-----------|----------|----------|\n",
    "|  `None` |   str    |  Ask a question: e.g. `\"What is the meaning of life?\"`  |\n",
    "|    str    |   urls   |  Do a web-search via `DuckDuckGo` based on the question.  |\n",
    "|   urls    | Documents|  Take the urls, scrape the corresponding web-pages, and convert to a list of `Document` objects  |\n",
    "| Documents | Documents|  Split the `Document` objects into smaller chunks (also `Document` objects)  |\n",
    "| Documents | `None` |  Store the Documents in a `Chroma` document index so that we can lookup the most relevant documents |\n",
    "| `None`  |   str    |  Return the original question `What is the meaning of life?` |\n",
    "|    str    |   str    |  Take the original question, lookup the most relevant documents, and construct the corresponding prompt with the documents injected into the prompt. |\n",
    "|    str    |   str    |  Pass the prompt to the chat model; receive a response |\n",
    "|    str    |   str    |  Construct a new prompt, containing the response and a request to summarize the response. |\n",
    "|    str    |   str    |  Pass the new prompt to the chat model; receive a response |\n",
    "\n",
    "**Notice how the output of one link matches the input of the next link. This means you could swap out any of the objects we use below with your own custom function or class and the only requirement is that the input/output matches for that particular link.**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial objects in chain\n",
    "\n",
    "Let's define the objects and functions we are going to include in the chain. We're including many links/tasks so that we can see how flexible our chain can be.\n",
    "\n",
    "See the [tools.ipynb](https://github.com/shane-kercheval/llm-chain/tree/main/examples/tools.ipynb) notebook for examples of how the various helper classes below (e.g. `DuckDuckGoSearch`, `scrape_url`, etc.) are used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick note on the `Value` object.\n",
    "\n",
    "The only bit of magic below is that we're using a `Value` object to cache the initial question, feed it into the web-search, and then inject it back into the chain at a later point (passing it to the prompt-template). A `Value` object is a callable that, when called with a value caches and returns the same value, and when called without a value, simply returns the previously cached value. When you understand what it's doing, it's not magic at all. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a value\n",
      "This is a value\n",
      "new value\n",
      "new value\n"
     ]
    }
   ],
   "source": [
    "from llm_chain.base import Value\n",
    "cache = Value()\n",
    "result = cache(\"This is a value\")  # calling the object with a value caches and returns the value\n",
    "print(result)  # the value in `result` is whatever was passed in to the Value object\n",
    "print(cache())  # calling the object without a parameter returns the cached value\n",
    "result = cache(\"new value\")\n",
    "print(result)\n",
    "print(cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llm_chain.base import Document, Chain, Value\n",
    "from llm_chain.models import OpenAIEmbeddings, OpenAIChat\n",
    "from llm_chain.tools import DuckDuckGoSearch, scrape_url, split_documents\n",
    "from llm_chain.indexes import ChromaDocumentIndex\n",
    "from llm_chain.prompt_templates import DocSearchTemplate\n",
    "\n",
    "# Seach DuckDuckGo based on initial question (passed into the chain)\n",
    "duckduckgo_search = DuckDuckGoSearch(top_n=3)\n",
    "\n",
    "# define a function that takes the links from the web-search, scrapes the web-pages,\n",
    "# and then creates Document objects from the text of each web-page\n",
    "def scrape_urls(search_results):\n",
    "    \"\"\"\n",
    "    For each url (i.e. `href` in `search_results`):\n",
    "    - extracts text\n",
    "    - replace new-lines with spaces\n",
    "    - create a Document object\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Document(content=re.sub(r'\\s+', ' ', scrape_url(x['href'])))\n",
    "        for x in search_results\n",
    "    ]\n",
    "\n",
    "# Embeddings model used for document index/search (the documents created from the web-search)\n",
    "embeddings_model = OpenAIEmbeddings(model_name='text-embedding-ada-002')\n",
    "document_index = ChromaDocumentIndex(embeddings_model=embeddings_model, n_results=3)\n",
    "# DocSearchTemplate uses the ChromaDocumentIndex object to search for the most relevant documents\n",
    "# (from the web-search) based on the intitial question (which it takes as an input)\n",
    "prompt_template = DocSearchTemplate(doc_index=document_index, n_docs=3)\n",
    "# Create a chat model without streaming.\n",
    "non_streaming_chat = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "# Create a chat model with streaming enabled via callback.\n",
    "streaming_chat = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    streaming_callback=lambda x: print(x.response, end='|'),\n",
    ")\n",
    "\n",
    "# A `Value` object is a simple caching mechanism. It's a callable that, when passed a value, it\n",
    "# caches and returns that value; and when called without a value, it returns the cached value.\n",
    "# Below, it's being used to cache the original question, feed the question into the web-search\n",
    "# (`DuckDuckGoSearch`), and then re-inject the question back in the chain and into the\n",
    "# prompt-template (`DocSearchTemplate`).\n",
    "question_1 = Value()\n",
    "# This simple function takes the response from the original chat model and creates a prompt that\n",
    "# asks the model to summarize the response.\n",
    "question_2 = lambda x: f'Summarize the following in less than 20 words: \"{x}\"'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Running the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| meaning| of| life| is| subjective| and| can| be| seen| as| creating| values|,| doing| good|,| benefiting| others|,| or| finding| purpose|.|"
     ]
    }
   ],
   "source": [
    "# each link is a callable where the output of one link is the input to the next\n",
    "chain = Chain(links=[\n",
    "    question_1,\n",
    "    duckduckgo_search,\n",
    "    scrape_urls,\n",
    "    split_documents,  # split web-pages into smaller chunks; defaults to chunk-size of 500\n",
    "    document_index,  # __call__ function calls add() if given a list of documents (which is returned by `split_documents`)\n",
    "    question_1,\n",
    "    prompt_template,\n",
    "    non_streaming_chat,\n",
    "    question_2,\n",
    "    streaming_chat,\n",
    "])\n",
    "# the value passed into `chain()` is passed to the `initial_question` object (which\n",
    "# is `Value` object and so it caches the value passed in and also returns it) which then gets\n",
    "# passed to the `duckduckgo_search` object, and so on.\n",
    "# the response of the final model is streamed, because our chat model has the streaming_callback\n",
    "# set, but it should also match the response returned\n",
    "response = chain(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is subjective and can be seen as creating values, doing good, benefiting others, or finding purpose."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total costs and usage\n",
    "\n",
    "The `Chain` object aggregates the costs/usage across all links that have a `history` property where that `history` property returns `UsageRecord` objects. In other words, any link that tracks its own history automatically gets counted towards the total usage within the Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:           $0.00552\n",
      "Total Tokens:    44,369\n",
      "Prompt Tokens:   567\n",
      "Response Tokens: 153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usage_string simply uses the properties in chain (chain.cost, chain.total_tokens, etc.)\n",
    "# to create a formatted string\n",
    "print(usage_string(chain))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "Similar to tracking the costs/usage, we can dig into the history at a more granular level.\n",
    "\n",
    "As you can see below, the `history` property returns a list containing:\n",
    "\n",
    "- a `SearchRecord` object capturing our original search query and results\n",
    "- two `EmbeddingsRecord` records; the first corresponds to getting the embeddings of all of the chunks from the web-pages; the second corresponds to getting the embedding of our original question so that we can find the most relevant chunks; the `EmbeddingsRecord` is a `UsageRecord` so it contains costs/usage\n",
    "- two `MessageRecord` records; the first corresponds to the first question (prompt & response) we made to the chat model; the second corresponds to our second query to the chat model asking it to summarize the first response; the `MessageRecord` is a `UsageRecord` so it contains costs/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[llm_chain.tools.SearchRecord,\n",
       " llm_chain.base.EmbeddingsRecord,\n",
       " llm_chain.base.EmbeddingsRecord,\n",
       " llm_chain.base.ExchangeRecord,\n",
       " llm_chain.base.ExchangeRecord]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in chain.history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-04 21:59:00.720; prompt: \"Answer the question at the end of the text as trut...\"; response: \"Based on the information provided, there are vario...\";  cost: $0.000870; total_tokens: 537; prompt_tokens: 408; response_tokens: 129; uuid: a092ccd7-05e1-45e2-a246-8a80e0cabe1c\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-04 21:59:01.343; prompt: \"Summarize the following in less than 20 words: \"Ba...\"; response: \"The meaning of life is subjective and can be seen ...\";  cost: $0.000286; total_tokens: 183; prompt_tokens: 159; response_tokens: 24; uuid: 71852ed6-d18e-46a4-accb-b61d3d9890c2\n"
     ]
    }
   ],
   "source": [
    "print(chain.history[3])\n",
    "mprint('---')\n",
    "print(chain.history[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `exchange_history`\n",
    "\n",
    "We can use the `exchange_history` property which simply returns all of the history items that are of type `ExchangeRecord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain.exchange_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-04 21:59:00.720; prompt: \"Answer the question at the end of the text as trut...\"; response: \"Based on the information provided, there are vario...\";  cost: $0.000870; total_tokens: 537; prompt_tokens: 408; response_tokens: 129; uuid: a092ccd7-05e1-45e2-a246-8a80e0cabe1c\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 2023-07-04 21:59:01.343; prompt: \"Summarize the following in less than 20 words: \"Ba...\"; response: \"The meaning of life is subjective and can be seen ...\";  cost: $0.000286; total_tokens: 183; prompt_tokens: 159; response_tokens: 24; uuid: 71852ed6-d18e-46a4-accb-b61d3d9890c2\n"
     ]
    }
   ],
   "source": [
    "print(chain.exchange_history[0])  # same as chain.history[3]\n",
    "mprint('---')\n",
    "print(chain.exchange_history[1])  # same as chain.history[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First prompt to the chat model\n",
    "\n",
    "You can see below that we used three chunks from our original web-search in the first prompt we sent to ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Answer the question at the end of the text as truthfully and accurately as possible, based on the following information provided.\n",
       "\n",
       "Here is the information:\n",
       "\n",
       "```\n",
       "\"leap\", arguing that life is full of absurdity, and one must make his and her own values in an indifferent world. One can live meaningfully (free of despair and anxiety) in an unconditional commitment to something finite and devotes that meaningful life to the commitment, despite the vulnerability inherent to doing so.[87] Arthur Schopenhauer answered: \"What is the meaning of life?\" by stating that one's life reflects one's will, and that the will (life) is an aimless, irrational, and painful\n",
       "\n",
       "lessons life offers us.[168] To find the meaning or purpose of life.[186][187] To find a reason to live.[188] To resolve the imbalance of the mind by understanding the nature of reality.[189] To do good, to do the right thing See also: ethics To leave the world as a better place than you found it.[161] To do your best to leave every situation better than you found it.[161] To benefit others.[11] To give more than you take.[161] To end suffering.[190][191][192] To create equality.[193][194][195]\n",
       "\n",
       "meaning of life is too profound to be known and understood.[189] You will never live if you are looking for the meaning of life.[161] The meaning of life is to forget about the search for the meaning of life.[161] Ultimately, a person should not ask what the meaning of their life is, but rather must recognize that it is they themselves who are asked. In a word, each person is questioned by life; and they can only answer to life by answering for their own life; to life they can only respond by\n",
       "```\n",
       "\n",
       "Here is the question:\n",
       "\n",
       "```\n",
       "What is the meaning of life?\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(chain.exchange_history[0].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First response from the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Based on the information provided, there are various perspectives on the meaning of life. Some argue that life is full of absurdity and that individuals must create their own values in an indifferent world. Others suggest that the meaning of life is to find purpose, to do good, to benefit others, to end suffering, or to create equality. However, it is also mentioned that the meaning of life may be too profound to be fully known and understood. Ultimately, it is suggested that individuals should recognize that they themselves are asked by life and can only answer for their own lives. Therefore, the meaning of life may vary depending on individual beliefs and perspectives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + chain.exchange_history[0].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second prompt to the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summarize the following in less than 20 words: \"Based on the information provided, there are various perspectives on the meaning of life. Some argue that life is full of absurdity and that individuals must create their own values in an indifferent world. Others suggest that the meaning of life is to find purpose, to do good, to benefit others, to end suffering, or to create equality. However, it is also mentioned that the meaning of life may be too profound to be fully known and understood. Ultimately, it is suggested that individuals should recognize that they themselves are asked by life and can only answer for their own lives. Therefore, the meaning of life may vary depending on individual beliefs and perspectives.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(chain.exchange_history[1].prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second response from the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> The meaning of life is subjective and can be seen as creating values, doing good, benefiting others, or finding purpose."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(\"> \" + chain.exchange_history[1].response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
