{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran in a docker container where the project directory (i.e. same directory as README.md) is located in `/code`, which is set below. If you run locally you'll need to set the path of your project directory accordingly.\n",
    "\n",
    "The `load_dotenv` function below loads all the variables found in the `.env` file as environment variables. You must have a `.env` file located in the project directory containing your OpenAI API key, in the following format.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code\n"
     ]
    }
   ],
   "source": [
    "%cd /code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def mprint(string: str, max_width: int = 80) -> None:\n",
    "    \"\"\"Print `string` with a maximum widgth.\"\"\"\n",
    "    wrapped_string = textwrap.fill(string, max_width)\n",
    "    print(wrapped_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example using `GPT-3.5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a philosophical question that has been debated by\n",
      "scholars, theologians, and philosophers for centuries. There is no one\n",
      "definitive answer to this question, as it can vary depending on one's beliefs,\n",
      "values, and experiences. Some people believe that the meaning of life is to seek\n",
      "happiness, while others believe it is to fulfill a specific purpose or destiny.\n",
      "Ultimately, the meaning of life is a personal and subjective concept that each\n",
      "individual must determine for themselves.\n"
     ]
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "model = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "response = model(\"What is the meaning of life?\")\n",
    "mprint(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stream the response by using a callback which takes a single parameter of type `StreamingRecord` which has a `response` property containing each token streamed. In the example below, we print simply print the response and end the printed result with the `|` character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| meaning| of| life| is| a| philosophical| question| that| has| been| debated| by| scholars|,| theolog|ians|,| and| philosophers| for| centuries|.| There| is| no| one| definitive| answer| to| this| question|,| as| it| can| vary| depending| on| one|'s| beliefs|,| values|,| and| experiences|.| Some| people| believe| that| the| meaning| of| life| is| to| seek| happiness|,| while| others| believe| it| is| to| fulfill| a| specific| purpose| or| destiny|.| Ultimately|,| the| meaning| of| life| is| a| personal| and| subjective| concept| that| each| individual| must| determine| for| themselves|.|"
     ]
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "model = OpenAIChat(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    streaming_callback=lambda x: print(x.response, end='|')\n",
    ")\n",
    "response = model(\"What is the meaning of life?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous example, the full text is returned at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a philosophical question that has been debated by\n",
      "scholars, theologians, and philosophers for centuries. There is no one\n",
      "definitive answer to this question, as it can vary depending on one's beliefs,\n",
      "values, and experiences. Some people believe that the meaning of life is to seek\n",
      "happiness, while others believe it is to fulfill a specific purpose or destiny.\n",
      "Ultimately, the meaning of life is a personal and subjective concept that each\n",
      "individual must determine for themselves.\n"
     ]
    }
   ],
   "source": [
    "mprint(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage & Cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_chain.models import OpenAIChat\n",
    "\n",
    "model = OpenAIChat(model_name='gpt-3.5-turbo')\n",
    "model(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:            $0.00005\n",
      "Total Tokens:          33\n",
      "Total Prompt Tokens:   26\n",
      "Total Response Tokens: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:            ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.prompt_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the same model/object again, the cost/usage will be incremented accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:            $0.00014\n",
      "Total Tokens:          90\n",
      "Total Prompt Tokens:   76\n",
      "Total Response Tokens: 14\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:            ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.prompt_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `history` property to get the prompt/response and cost/usage for each of the messages used by the model/object. \n",
    "\n",
    "There are two `MessageRecord` items in the list. The first item corresponds to the first question and the second item corresponds to the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MessageRecord(uuid='3d2d5acc-4da7-4385-b941-a3840b5fc929', timestamp='2023-06-23 04:55:22.213', metadata={'model_name': 'gpt-3.5-turbo'}, total_tokens=33, cost=5.3e-05, prompt='What is the capital of France?', response='The capital of France is Paris.', prompt_tokens=26, response_tokens=7),\n",
       " MessageRecord(uuid='fa7e5d93-a2f7-4498-a946-a1f57c68624a', timestamp='2023-06-23 04:55:25.517', metadata={'model_name': 'gpt-3.5-turbo'}, total_tokens=57, cost=8.900000000000001e-05, prompt='What is the capital of Germany?', response='The capital of Germany is Berlin.', prompt_tokens=50, response_tokens=7)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is the capital of France?\n",
      "prompt: 7\n",
      "Total Cost:            $0.00005\n",
      "Total Tokens:          33\n",
      "Total Prompt Tokens:   26\n",
      "Total Response Tokens: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompt: {model.history[0].prompt}\")\n",
    "print(f\"prompt: {model.history[0].response_tokens}\")\n",
    "print(f\"Total Cost:            ${model.history[0].cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.history[0].total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.history[0].prompt_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.history[0].response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is the capital of Germany?\n",
      "prompt: 7\n",
      "Total Cost:            $0.00009\n",
      "Total Tokens:          57\n",
      "Total Prompt Tokens:   50\n",
      "Total Response Tokens: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"prompt: {model.history[1].prompt}\")\n",
    "print(f\"prompt: {model.history[1].response_tokens}\")\n",
    "print(f\"Total Cost:            ${model.history[1].cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.history[1].total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.history[1].prompt_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.history[1].response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
